{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the libraries and define global variable (like size of the gameboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-19 15:15:31.099649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "global Xdim\n",
    "global Ydim\n",
    "global WinNumber  # Number in a row necessary for a win, typically 4\n",
    "global RewardForWin\n",
    "global MAX\n",
    "MAX = 1e308\n",
    "global game_number_list\n",
    "global mean_reward_list\n",
    "\n",
    "# Connect-4 is usually played on a 7x6 gameboard\n",
    "Xdim = 7\n",
    "action_space_size=Xdim\n",
    "Ydim = 6\n",
    "WinNumber = 4\n",
    "# Define your incentives\n",
    "RewardForWin = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # How many experiences to use for each training step. Mnih2015=32\n",
    "\n",
    "#update_freq = 1\n",
    "#update_freq = 2\n",
    "#update_freq = 4 # Actually, this is frequency inverted.  How many steps between training rounds. Mnih2015=4\n",
    "update_freq = 8\n",
    "#update_freq = 16\n",
    "\n",
    "#targetQ_y = 0.999\n",
    "#targetQ_y = 0.997\n",
    "#targetQ_y = 0.990 # Discount factor on the target Q-values Mnih2015=0.99\n",
    "#targetQ_y = 0.970\n",
    "targetQ_y = 0.400\n",
    "\n",
    "pre_train_steps = 500 # How many steps of random actions before training begins\n",
    "startE = 1 # Starting chance of random action Mnih2015=1\n",
    "endE = 0.1 # Final chance of random action Mnih2015=0.1\n",
    "annealing_time = 0.6 # Percent of games to taper off the exploration: 0 = go immediately to endE, 1 = taper exploration during 100% of games, Mnih2015=.02\n",
    "\n",
    "h_size = 512 # The number of units in the hidden layer. Mnih2015=512\n",
    "\n",
    "learning_rate = 0.0003 # Mnih2015 = .00025\n",
    "\n",
    "#training_games = 10000 # How many games to train the network with\n",
    "#training_games = 100000 # 90 minutes\n",
    "training_games = 500000 # overnight\n",
    "training_games = 1\n",
    "\n",
    "model_path = \"./Trainings_500Ktraining/models4/dqn_training_reducedLossPenalties\" # The path to save our model to.\n",
    "summary_path = './summaries4/dqn' # The path to save summary statistics to.\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model\n",
    "\n",
    "PRETRAIN_WINS = False\n",
    "Pretrain_Scale = 5\n",
    "PRETRAIN_THREATS = False \n",
    "MIRRORED_TRAINING = False\n",
    "REWARD_FOR_THREAT = False\n",
    "MINIMAX_TRAINING = False # Not programmed yet\n",
    "REWARD_EVERY_MOVE = True\n",
    "TRAIN_MISSED_WINS = False\n",
    "TRAIN_MISSED_BLOCKS = False\n",
    "PRIORITIZED_EXPERIENCE = False\n",
    "TRAIN_AFTER_TESTGAMES = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define sub-routines for displaying the board and checking for a win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# DISPLAY THE GAMEBOARD WITH PROPER ORIENTATION\n",
    "#\n",
    "def display_gameboard(gameboard):\n",
    "    ylen = len(gameboard[0])\n",
    "    xlen = len(gameboard)\n",
    "    #print(xlen,ylen)\n",
    "    gameboard_for_display = np.zeros((xlen,ylen),dtype=int)\n",
    "    for i in range(ylen):\n",
    "        gameboard_for_display[:,i] = gameboard[:,ylen-1-i]\n",
    "    print(gameboard_for_display.T)\n",
    "    #print(\"\\n\",gameboard_for_display.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  CHECK TO SEE IF THE LATEST MOVE WINS THE GAME\n",
    "#\n",
    "def check_for_win_vector(vector):\n",
    "    reward = 0\n",
    "    for start in range(0,len(vector)-WinNumber+1):\n",
    "        win_at_this_startpoint = 1\n",
    "        for i in range(start, start+WinNumber):\n",
    "            if (vector[i] != 1):\n",
    "                win_at_this_startpoint = 0\n",
    "        if (win_at_this_startpoint):\n",
    "            reward=1\n",
    "            #reward=RewardForWin\n",
    "            return(reward)\n",
    "    return(reward)\n",
    "    \n",
    "def check_for_win(gameboard,x,y):\n",
    "    horizontalVector = gameboard.T[y]\n",
    "    #print('horizontalVector=',horizontalVector)\n",
    "    reward=check_for_win_vector(horizontalVector)\n",
    "    if reward:\n",
    "        #print('horizontalVector=',horizontalVector)\n",
    "        return(reward)\n",
    "    \n",
    "    verticalVector = gameboard[x]\n",
    "    #print('verticalVector= ',verticalVector)\n",
    "    reward=check_for_win_vector(verticalVector)\n",
    "    if reward:\n",
    "        #print('verticalVector= ',verticalVector)\n",
    "        return(reward)\n",
    "    \n",
    "    diagonalVectorUp=np.zeros(max(Xdim,Ydim),dtype=int)\n",
    "    diagonalVectorDn=np.zeros(max(Xdim,Ydim),dtype=int)\n",
    "    for i in range(0,max(Xdim,Ydim)):\n",
    "        ycoord = i\n",
    "        xcoordUp = x-(y-i) # create vector running down-left to up-right\n",
    "        if (i < Ydim) and (xcoordUp in range(0,Xdim)):\n",
    "            diagonalVectorUp[i]= gameboard[xcoordUp][ycoord]\n",
    "        xcoordDn = x+(y-i) # create vector running up-left to down-right\n",
    "        if (i < Ydim) and (xcoordDn in range(0,Xdim)):\n",
    "            diagonalVectorDn[-1-i]= gameboard[xcoordDn][ycoord]\n",
    "       \n",
    "    #print('diagonalVectorUp = ',diagonalVectorUp)\n",
    "    reward=check_for_win_vector(diagonalVectorUp)\n",
    "    if reward:\n",
    "        #print('diagonalVectorUp = ',diagonalVectorUp)\n",
    "        return(reward)\n",
    "    \n",
    "    #print('diagonalVectorDn = ',diagonalVectorDn)\n",
    "    reward=check_for_win_vector(diagonalVectorDn)\n",
    "    if reward:\n",
    "        #print('diagonalVectorDn = ',diagonalVectorDn)\n",
    "        return(reward)\n",
    "    \n",
    "    return(0) # reward= 0 if no win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_possible_win(gameboard):\n",
    "    possible_win=0\n",
    "    win_column=0\n",
    "    for column in range(Xdim):\n",
    "        gameboard_possible=gameboard.copy()\n",
    "        if(gameboard[column,-1]==0):\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column\n",
    "            gameboard_possible[column,row] = 1\n",
    "            if check_for_win(gameboard_possible,column,row):\n",
    "                possible_win+=1\n",
    "                win_column=column\n",
    "                #print('\\nPossible win in column', win_column)\n",
    "                #display_gameboard(gameboard_possible)\n",
    "    return(possible_win,win_column)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_possible_win_verbose(gameboard):\n",
    "    possible_win=0\n",
    "    win_column=0\n",
    "    for column in range(Xdim):\n",
    "        gameboard_possible=gameboard.copy()\n",
    "        if(gameboard[column,-1]==0):\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column\n",
    "            gameboard_possible[column,row] = 1\n",
    "            if check_for_win(gameboard_possible,column,row):\n",
    "                print('POSSIBLE WIN HERE')\n",
    "                display_gameboard(gameboard_possible)\n",
    "                possible_win+=1\n",
    "                win_column=column\n",
    "                #print('\\nPossible win in column', win_column)\n",
    "                #display_gameboard(gameboard_possible)\n",
    "    return(possible_win,win_column)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self ,h_size, num_actions, lr, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            # The network recieves a frame from the game, flattened into an array.\n",
    "            # It then resizes it and processes it through two convolutional layers.\n",
    "            #self.observation_input =  tf.placeholder(shape=[None, 12, 12, 3],dtype=tf.float32)\n",
    "            self.observation_input =  tf.placeholder(shape=[None, Xdim, Ydim, 3],dtype=tf.float32) #new\n",
    "            \n",
    "            self.conv1 = slim.conv2d(self.observation_input, 64, \n",
    "                                     kernel_size=[3,3], stride=[2,2], \n",
    "                                     biases_initializer=None,\n",
    "                                     activation_fn=tf.nn.elu)\n",
    "            self.conv2 = slim.conv2d(self.conv1, 64, \n",
    "                                     kernel_size=[3,3], \n",
    "                                     stride=[2,2], \n",
    "                                     biases_initializer=None,\n",
    "                                     activation_fn=tf.nn.elu)\n",
    "\n",
    "            # We take the output from the final convolutional layer \n",
    "            # and split it into separate advantage and value streams.\n",
    "            self.hidden = slim.fully_connected(slim.flatten(self.conv2), \n",
    "                                               h_size, activation_fn=tf.nn.elu)\n",
    "            self.advantage = slim.fully_connected(self.hidden, num_actions, activation_fn=None,\n",
    "                                                  biases_initializer=None)\n",
    "            self.value = slim.fully_connected(self.hidden, 1, activation_fn=None,\n",
    "                                                  biases_initializer=None)\n",
    "\n",
    "            # Task 1: Combine advantage and vaule together to get the final Q-values.\n",
    "            self.q_out = self.value + tf.subtract(self.advantage, \n",
    "                                                  tf.reduce_mean(self.advantage,axis=1, keep_dims=True))\n",
    "            # Task 2: Select the best action given q_out\n",
    "            self.predict = tf.argmax(self.q_out,1)\n",
    "\n",
    "            # Below we obtain the loss by taking the sum of squares difference \n",
    "            # between the target and prediction Q values.\n",
    "            self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,num_actions,dtype=tf.float32)\n",
    "\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.q_out, self.actions_onehot), axis=1)\n",
    "\n",
    "            # Task 3: Compute the TD error\n",
    "            self.td_error = tf.square(self.targetQ - self.Q)\n",
    "            self.loss = tf.reduce_mean(self.td_error)\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            self.update = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_graph(from_scope, to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Q-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards decided by state, not by action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Train the network \n",
    "# Definitions\n",
    "action_space_size = Xdim\n",
    "tf.reset_default_graph()\n",
    "#mainQN = Qnetwork(h_size, action_space_size, learning_rate, \"main\") # variable action_space_size\n",
    "#targetQN = Qnetwork(h_size, action_space_size, learning_rate, \"target\") # variable action_space_size\n",
    "mainQN = Qnetwork(h_size, 1, learning_rate, \"main\")\n",
    "targetQN = Qnetwork(h_size, 1, learning_rate, \"target\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "    \n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "update_target_ops = update_target_graph(\"main\", \"target\")\n",
    "\n",
    "myBuffer = experience_buffer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-19 15:15:37.417906\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Save the old code\\n#\\n# Pre-train on all the win patterns\\n#\\ndef prep_for_buffer(gameboard,reward):\\n            #display_gameboard(gameboard)\\n            done=1\\n            observations = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) #new\\n            observation = np.concatenate([observations, observations, observations], axis=2) #new\\n            observations_1 = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) #new\\n            observation_1 = observation[:, :, 1:] #new\\n            observation_1 = np.concatenate([observation_1, observations_1], axis=2) #new\\n            preppedBuffer = np.reshape(np.array([observation,0,reward,observation_1,done]),[1,5])\\n            return(preppedBuffer)\\n            \\nif (PRETRAIN_WINS):\\n    empty_gameboard = np.zeros((Xdim,Ydim),dtype=int)+2\\n    #reward=RewardForWin\\n    pretrain_batch_size= 0\\n\\n    # 6x4 = 24 horizontal wins (or losses)\\n    #print('HORIZONTAL')\\n    for y in range(Ydim):\\n        for x in range(Xdim-WinNumber+1):\\n            gameboard = empty_gameboard.copy()\\n            for i in range(WinNumber):\\n                gameboard[x+i,y]=1\\n            preppedBuffer = prep_for_buffer(gameboard,5*RewardForWin)\\n            myBuffer.add(preppedBuffer)\\n            pretrain_batch_size+=1\\n            preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\\n            myBuffer.add(preppedBuffer)\\n            pretrain_batch_size+=1\\n\\n    # 7x3 = 21 vertical wins (or losses)\\n    #print('VERTICAL')\\n    for x in range(Xdim):\\n        for y in range(Ydim-WinNumber+1):\\n            gameboard = empty_gameboard.copy()\\n            for i in range(WinNumber):\\n                gameboard[x,y+i]=1\\n            preppedBuffer = prep_for_buffer(gameboard,5*RewardForWin)\\n            myBuffer.add(preppedBuffer)\\n            pretrain_batch_size+=1\\n            preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\\n            myBuffer.add(preppedBuffer)\\n            pretrain_batch_size+=1\\n\\n    # 12 diagonal wins (each way) = 24 diagonal wins (or losses)\\n    #print('DIAGONAL') \\n    for column in range(Xdim-WinNumber+1):\\n        for row in range(Ydim-WinNumber+1):\\n            gameboard = empty_gameboard.copy()\\n            for i in range(WinNumber):\\n                gameboard[column+i, row+i]= 1\\n            preppedBuffer = prep_for_buffer(gameboard,5*RewardForWin)\\n            myBuffer.add(preppedBuffer)\\n            pretrain_batch_size+=1\\n            preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\\n            myBuffer.add(preppedBuffer)\\n            pretrain_batch_size+=1\\n        \\n            gameboard = np.flip(gameboard,0)\\n            preppedBuffer = prep_for_buffer(gameboard,5*RewardForWin)\\n            myBuffer.add(preppedBuffer)\\n            pretrain_batch_size+=1\\n            preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\\n            myBuffer.add(preppedBuffer)\\n            pretrain_batch_size+=1\\n\\n# 69 total different ways to win (and 69 ways to lose)\\n#print(pretrain_batch_size)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Save the old code\n",
    "#\n",
    "# Pre-train on all the win patterns\n",
    "#\n",
    "def prep_for_buffer(gameboard,reward):\n",
    "            #display_gameboard(gameboard)\n",
    "            done=1\n",
    "            observations = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) #new\n",
    "            observation = np.concatenate([observations, observations, observations], axis=2) #new\n",
    "            observations_1 = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) #new\n",
    "            observation_1 = observation[:, :, 1:] #new\n",
    "            observation_1 = np.concatenate([observation_1, observations_1], axis=2) #new\n",
    "            preppedBuffer = np.reshape(np.array([observation,0,reward,observation_1,done]),[1,5])\n",
    "            return(preppedBuffer)\n",
    "            \n",
    "if (PRETRAIN_WINS):\n",
    "    empty_gameboard = np.zeros((Xdim,Ydim),dtype=int)+2\n",
    "    #reward=RewardForWin\n",
    "    pretrain_batch_size= 0\n",
    "\n",
    "    # 6x4 = 24 horizontal wins (or losses)\n",
    "    #print('HORIZONTAL')\n",
    "    for y in range(Ydim):\n",
    "        for x in range(Xdim-WinNumber+1):\n",
    "            gameboard = empty_gameboard.copy()\n",
    "            for i in range(WinNumber):\n",
    "                gameboard[x+i,y]=1\n",
    "            preppedBuffer = prep_for_buffer(gameboard,5*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "            preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "\n",
    "    # 7x3 = 21 vertical wins (or losses)\n",
    "    #print('VERTICAL')\n",
    "    for x in range(Xdim):\n",
    "        for y in range(Ydim-WinNumber+1):\n",
    "            gameboard = empty_gameboard.copy()\n",
    "            for i in range(WinNumber):\n",
    "                gameboard[x,y+i]=1\n",
    "            preppedBuffer = prep_for_buffer(gameboard,5*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "            preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "\n",
    "    # 12 diagonal wins (each way) = 24 diagonal wins (or losses)\n",
    "    #print('DIAGONAL') \n",
    "    for column in range(Xdim-WinNumber+1):\n",
    "        for row in range(Ydim-WinNumber+1):\n",
    "            gameboard = empty_gameboard.copy()\n",
    "            for i in range(WinNumber):\n",
    "                gameboard[column+i, row+i]= 1\n",
    "            preppedBuffer = prep_for_buffer(gameboard,5*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "            preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "        \n",
    "            gameboard = np.flip(gameboard,0)\n",
    "            preppedBuffer = prep_for_buffer(gameboard,5*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "            preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "\n",
    "# 69 total different ways to win (and 69 ways to lose)\n",
    "#print(pretrain_batch_size)'''\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Pre-train on all the win patterns\n",
    "#\n",
    "def prep_for_buffer(gameboard,reward):\n",
    "            #display_gameboard(gameboard)\n",
    "            done=1\n",
    "            observations = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) #new\n",
    "            observation = np.concatenate([observations, observations, observations], axis=2) #new\n",
    "            observations_1 = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) #new\n",
    "            observation_1 = observation[:, :, 1:] #new\n",
    "            observation_1 = np.concatenate([observation_1, observations_1], axis=2) #new\n",
    "            preppedBuffer = np.reshape(np.array([observation,0,reward,observation_1,done]),[1,5])\n",
    "            return(preppedBuffer)\n",
    "\n",
    "pretrain_batch_size = 0\n",
    "            \n",
    "if (PRETRAIN_WINS):\n",
    "    empty_gameboard = np.zeros((Xdim,Ydim),dtype=int)+2\n",
    "    #reward=RewardForWin\n",
    "    #pretrain_batch_size= 0\n",
    "\n",
    "    # 6x4 = 24 horizontal wins (or losses)\n",
    "    #print('HORIZONTAL')\n",
    "    for y in range(Ydim):\n",
    "        for x in range(Xdim-WinNumber+1):\n",
    "            gameboard = empty_gameboard.copy()\n",
    "            for i in range(WinNumber):\n",
    "                gameboard[x+i,y]=1\n",
    "            preppedBuffer = prep_for_buffer(gameboard,Pretrain_Scale*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "            if (MIRRORED_TRAINING):\n",
    "                preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\n",
    "                myBuffer.add(preppedBuffer)\n",
    "                pretrain_batch_size+=1\n",
    "\n",
    "    # 7x3 = 21 vertical wins (or losses)\n",
    "    #print('VERTICAL')\n",
    "    for x in range(Xdim):\n",
    "        for y in range(Ydim-WinNumber+1):\n",
    "            gameboard = empty_gameboard.copy()\n",
    "            for i in range(WinNumber):\n",
    "                gameboard[x,y+i]=1\n",
    "            preppedBuffer = prep_for_buffer(gameboard,Pretrain_Scale*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "            if (MIRRORED_TRAINING):\n",
    "                preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\n",
    "                myBuffer.add(preppedBuffer)\n",
    "                pretrain_batch_size+=1\n",
    "\n",
    "    # 12 diagonal wins (each way) = 24 diagonal wins (or losses)\n",
    "    #print('DIAGONAL') \n",
    "    for column in range(Xdim-WinNumber+1):\n",
    "        for row in range(Ydim-WinNumber+1):\n",
    "            gameboard = empty_gameboard.copy()\n",
    "            for i in range(WinNumber):\n",
    "                gameboard[column+i, row+i]= 1\n",
    "            preppedBuffer = prep_for_buffer(gameboard,Pretrain_Scale*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "            if (MIRRORED_TRAINING):\n",
    "                preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\n",
    "                myBuffer.add(preppedBuffer)\n",
    "                pretrain_batch_size+=1\n",
    "        \n",
    "            gameboard = np.flip(gameboard,0)\n",
    "            preppedBuffer = prep_for_buffer(gameboard,Pretrain_Scale*RewardForWin)\n",
    "            myBuffer.add(preppedBuffer)\n",
    "            pretrain_batch_size+=1\n",
    "            if (MIRRORED_TRAINING):\n",
    "                preppedBuffer = prep_for_buffer(-1*gameboard,-1*RewardForWin)\n",
    "                myBuffer.add(preppedBuffer)\n",
    "                pretrain_batch_size+=1\n",
    "\n",
    "# 69 total different ways to win (and 69 ways to lose)\n",
    "#print(pretrain_batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Pre-train on all the threat patterns (one move away from a victory)\n",
    "#\n",
    "\n",
    "threat_reward_scale = 0.5\n",
    "if (PRETRAIN_THREATS):\n",
    "    empty_gameboard = np.zeros((Xdim,Ydim),dtype=int)+2\n",
    "\n",
    "    # 6x4 = 24 horizontal wins (or losses)\n",
    "    #print('HORIZONTAL')\n",
    "    for y in range(Ydim):\n",
    "        for x in range(Xdim-WinNumber+1):\n",
    "            gameboard = empty_gameboard.copy()\n",
    "            for i in range(WinNumber):\n",
    "                gameboard[x+i,y]=1\n",
    "            for i in range(WinNumber):\n",
    "                gameboard_threat = gameboard.copy()\n",
    "                gameboard_threat[x+i,y]= 0\n",
    "                #display_gameboard(gameboard_threat)\n",
    "                preppedBuffer = prep_for_buffer(gameboard_threat,threat_reward_scale*RewardForWin)\n",
    "                myBuffer.add(preppedBuffer)\n",
    "                pretrain_batch_size+=1\n",
    "                if (MIRRORED_TRAINING):\n",
    "                    preppedBuffer = prep_for_buffer(-1*gameboard_threat,-1*RewardForWin)\n",
    "                    myBuffer.add(preppedBuffer)\n",
    "                    pretrain_batch_size+=1\n",
    "\n",
    "    # 7x3 = 21 vertical wins (or losses)\n",
    "    #print('VERTICAL')\n",
    "    for x in range(Xdim):\n",
    "        for y in range(Ydim-WinNumber+1):\n",
    "            gameboard = empty_gameboard.copy()\n",
    "            for i in range(WinNumber):\n",
    "                gameboard[x,y+i]=1\n",
    "            for i in range(WinNumber):\n",
    "                gameboard_threat = gameboard.copy()\n",
    "                gameboard_threat[x,y+i]= 0\n",
    "                #display_gameboard(gameboard_threat)\n",
    "                preppedBuffer = prep_for_buffer(gameboard_threat,threat_reward_scale*RewardForWin)\n",
    "                myBuffer.add(preppedBuffer)\n",
    "                pretrain_batch_size+=1\n",
    "                if (MIRRORED_TRAINING):\n",
    "                    preppedBuffer = prep_for_buffer(-1*gameboard_threat,-1*RewardForWin)\n",
    "                    myBuffer.add(preppedBuffer)\n",
    "                    pretrain_batch_size+=1\n",
    "\n",
    "    # 12 diagonal wins (each way) = 24 diagonal wins (or losses)\n",
    "    #print('DIAGONAL') \n",
    "    for column in range(Xdim-WinNumber+1):\n",
    "        for row in range(Ydim-WinNumber+1):\n",
    "            gameboard = empty_gameboard.copy()\n",
    "            for i in range(WinNumber):\n",
    "                gameboard[column+i, row+i]= 1\n",
    "            for i in range(WinNumber):\n",
    "                # Diagonal rising to the right\n",
    "                gameboard_threat = gameboard.copy()\n",
    "                gameboard_threat[column+i,row+i]= 0 \n",
    "                #display_gameboard(gameboard_threat)\n",
    "                preppedBuffer = prep_for_buffer(gameboard_threat,threat_reward_scale*RewardForWin)\n",
    "                myBuffer.add(preppedBuffer)\n",
    "                pretrain_batch_size+=1\n",
    "                if (MIRRORED_TRAINING):\n",
    "                    preppedBuffer = prep_for_buffer(-1*gameboard_threat,-1*RewardForWin)\n",
    "                    myBuffer.add(preppedBuffer)\n",
    "                    pretrain_batch_size+=1\n",
    "        \n",
    "                # Diagonal rising to the left (horizontal mirroring)\n",
    "                gameboard_threat = np.flip(gameboard_threat,0)\n",
    "                #display_gameboard(gameboard_threat)\n",
    "                preppedBuffer = prep_for_buffer(gameboard_threat,threat_reward_scale*RewardForWin)\n",
    "                myBuffer.add(preppedBuffer)\n",
    "                pretrain_batch_size+=1\n",
    "                if (MIRRORED_TRAINING):\n",
    "                    preppedBuffer = prep_for_buffer(-1*gameboard_threat,-1*RewardForWin)\n",
    "                    myBuffer.add(preppedBuffer)\n",
    "                    pretrain_batch_size+=1\n",
    "\n",
    "# 276 total different ways to threaten (and 276 ways to be threatened)\n",
    "print(pretrain_batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "WARNING:tensorflow:From C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./Trainings_500Ktraining/models4/dqn_training_rewardEveryMove-Copy\\model-499999.cptk\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Train the network\n",
    "# Training\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "reward_history = np.zeros(Xdim*Ydim,dtype=float)\n",
    "Qestimate_history = np.zeros(Xdim*Ydim,dtype=float)\n",
    "random_move_history = np.zeros(Xdim*Ydim,dtype=bool)\n",
    "episode_lengths = []\n",
    "episode_rewards = []\n",
    "losses = []\n",
    "total_steps = 0\n",
    "list_counter= 0\n",
    "game_number_list = [0]*100\n",
    "mean_reward_list = [0]*100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    # PRE-TRAIN ON WINNING/LOSING GAMEBOARDS\n",
    "    #pretrain_batch_size = 64\n",
    "    if (PRETRAIN_WINS) or (PRETRAIN_THREATS):\n",
    "      for i in range(500):\n",
    "        # Train on winning/losing boards\n",
    "        trainBatch = myBuffer.sample(pretrain_batch_size)\n",
    "        # Below we perform the Double-DQN update to the target Q-values\n",
    "        Q1 = sess.run(mainQN.predict, feed_dict={mainQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "        Q2 = sess.run(targetQN.q_out, feed_dict={targetQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "        end_multiplier = -(trainBatch[:,4] - 1)\n",
    "        doubleQ = Q2[range(pretrain_batch_size),Q1]\n",
    "        targetQ = trainBatch[:,2] + (targetQ_y*doubleQ * end_multiplier)\n",
    "        # Update the network with our target values.\n",
    "        _, q_loss = sess.run([mainQN.update, mainQN.loss],\n",
    "        feed_dict={mainQN.observation_input:np.stack(trainBatch[:,0], axis=0),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "        losses.append(q_loss)      \n",
    "    \n",
    "    # This is the loop for each new game\n",
    "    for game_number in range(training_games):\n",
    "        training_percent = min(1,game_number/(training_games*annealing_time))\n",
    "        Epsilon = startE*(1-training_percent) + endE*(training_percent) # Annealing: Decay the epsilon over time\n",
    "        episodeBuffer = experience_buffer()\n",
    "        \n",
    "        # reset gameboard\n",
    "        gameboard = np.zeros((Xdim,Ydim),dtype=int) #new\n",
    "        gameboard_history = np.zeros((Xdim*Ydim,Xdim,Ydim),dtype=int) \n",
    "        observations = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) #new\n",
    "        observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        \n",
    "        # This is the main loop of the game, each turn\n",
    "        while not done:\n",
    "            gameboard *=-1 # switch sides every turn \n",
    "            #gameboard_history[episode_steps] = gameboard \n",
    "            # Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if (np.random.rand(1) < Epsilon or total_steps < pre_train_steps) and train_model:\n",
    "                random_move_history[episode_steps] = True\n",
    "                column = np.random.randint(0,action_space_size)\n",
    "                while (gameboard[column,Ydim-1]):  # if the column is full, pick a new column #new\n",
    "                    column = np.random.randint(0,action_space_size) \n",
    "            else:\n",
    "                random_move_history[episode_steps] = False\n",
    "                Qstate_value = [0.0 for i in range(Xdim)]\n",
    "                for column_possible in range(Xdim):\n",
    "                    if gameboard[column_possible,-1]:\n",
    "                        Qstate_value[column_possible]= -MAX # don't allow an illegal move into a full column\n",
    "                    else:\n",
    "                        row_possible = list(gameboard[column_possible]).index(0) # Find the lowest open slot in the chosen column #new\n",
    "                        gameboard_possible = gameboard.copy()\n",
    "                        gameboard_possible[column_possible,row_possible] = 1  \n",
    "                        observations = np.reshape(gameboard_possible, [gameboard_possible.shape[0], gameboard_possible.shape[1], 1]) #new\n",
    "                        observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "                        Qvalue = sess.run(mainQN.q_out,feed_dict={mainQN.observation_input:[observation]})[0]                    \n",
    "                        Qstate_value[column_possible]=Qvalue[0]\n",
    "                \n",
    "                Qstate_value_max = max(Qstate_value)\n",
    "                Qestimate_history[episode_steps] = Qstate_value_max\n",
    "                count = Qstate_value.count(Qstate_value_max)\n",
    "                if count>1:\n",
    "                    column = random.randint(0,Xdim-1)\n",
    "                    while Qstate_value[column] != Qstate_value_max:\n",
    "                        column=random.randint(0,Xdim-1)\n",
    "                else:\n",
    "                    column = Qstate_value.index(Qstate_value_max) # Find the first/only occurence of the max value         \n",
    "                    \n",
    "            if not train_model and np.random.rand(1) < endE:\n",
    "                column = np.random.randint(0,action_space_size)\n",
    "                while (gameboard[column,Ydim-1]):  # if the column is full, pick a new column \n",
    "                    column = np.random.randint(0,action_space_size) \n",
    "                           \n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column \n",
    "            gameboard[column,row] = 1\n",
    "            gameboard_history[episode_steps] = gameboard \n",
    "           \n",
    "            reward=RewardForWin*check_for_win(gameboard,column,row) # REWARD A VICTORY\n",
    "            if (reward >0) or (0 not in gameboard): \n",
    "                done = 1\n",
    "            else: \n",
    "                done = 0\n",
    "                loss, loss_column = check_for_possible_win(-1*gameboard) # NEGATIVE REWARD FOR GIVING A WIN OPPORTUNITY TO OPPONENT\n",
    "                reward= -loss*RewardForWin\n",
    "                if (REWARD_FOR_THREAT) and (reward==0):\n",
    "                    possible_win,possible_win_column=check_for_possible_win(gameboard)\n",
    "                    reward = RewardForWin*min(2,possible_win)/3 # Semi-positive reward for creating a possible-winning threat (fork=2/3*RewardForWin)\n",
    "            reward_history[episode_steps] = reward\n",
    "            \n",
    "            observations_1 = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1])\n",
    "            observation_1 = observation[:, :, 1:]\n",
    "            observation_1 = np.concatenate([observation_1, observations_1], axis=2)            \n",
    "            # Save the experience to our episode buffer.\n",
    "            #episodeBuffer.add(np.reshape(np.array([observation,column,reward,observation_1,done]),[1,5])) # Change this to remove action?\n",
    "            episodeBuffer.add(np.reshape(np.array([observation,0,reward,observation_1,done]),[1,5])) # Changed this to remove action?\n",
    "    \n",
    "            total_steps += 1\n",
    "            # Run periodic training\n",
    "            if total_steps > pre_train_steps and train_model:\n",
    "                if total_steps % 1000 == 0:\n",
    "                    sess.run(update_target_ops)\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    # Get a random batch of experiences.\n",
    "                    trainBatch = myBuffer.sample(batch_size) \n",
    "                    # Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict, \n",
    "                                  feed_dict={mainQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "                    Q2 = sess.run(targetQN.q_out, \n",
    "                                  feed_dict={targetQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (targetQ_y*doubleQ * end_multiplier)\n",
    "                    # Update the network with our target values.\n",
    "                    _, q_loss = sess.run([mainQN.update, mainQN.loss],\n",
    "                        feed_dict={mainQN.observation_input:np.stack(trainBatch[:,0], axis=0),\n",
    "                                   mainQN.targetQ:targetQ, \n",
    "                                   mainQN.actions:trainBatch[:,1]})\n",
    "                    #print('q_loss = ',q_loss) # deleteme\n",
    "                    losses.append(q_loss)\n",
    "            #print('losses = ',losses) # deleteme\n",
    "            episode_reward += reward\n",
    "            observation = observation_1\n",
    "            episode_steps+=1  # Last thing before looping back for the next move of the game\n",
    "            \n",
    "        # At the end of each game, Add all the gameboard steps to the buffer\n",
    "        #print('EpisodeBuffer old\\n',episodeBuffer.buffer) #new\n",
    "        episodeBuffer = experience_buffer() \n",
    "        # Determine the reward for the gameboard at each move\n",
    "        for step in range(episode_steps): \n",
    "            if reward_history[step]:\n",
    "                reward = reward_history[step]  # Rewards from win and possible losses\n",
    "            else:\n",
    "              if (REWARD_EVERY_MOVE):\n",
    "                reward = RewardForWin/(episode_steps-step) # This would give attenated rewards\n",
    "                if (step % 2 == episode_steps % 2):\n",
    "                    reward *=-1   # Positive rewards for the winning side/negative rewards for the losing side.\n",
    "            # Convert the gameboard to an obeservation to append to the episode buffer\n",
    "            gameboard = gameboard_history[step] \n",
    "            observations = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) \n",
    "            observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "            observations_1 = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) \n",
    "            observation_1 = observation[:, :, 1:] \n",
    "            observation_1 = np.concatenate([observation_1, observations_1], axis=2) \n",
    "            if reward:\n",
    "                replay=1\n",
    "                if (random_move_history[step]==False) and (PRIORITIZED_EXPERIENCE == True):\n",
    "                    if (abs(reward-Qestimate_history[step])>50):\n",
    "                        replay =2\n",
    "                        if (abs(reward-Qestimate_history[step])>100):\n",
    "                            replay=4\n",
    "                    #print('Qestimate =',Qestimate_history[step],' reward = ',reward,' replay = ',replay) #deleteme\n",
    "                for i in range(replay):\n",
    "                    episodeBuffer.add(np.reshape(np.array([observation,0,reward,observation_1,done]),[1,5]))\n",
    "                    if (MIRRORED_TRAINING):\n",
    "                        episodeBuffer.add(np.reshape(np.array([-1*observation,0,-reward,-1*observation_1,done]),[1,5])) # Try mirroring the negative reward     \n",
    "            #else:\n",
    "                #print('No reward for this move')\n",
    "            if (TRAIN_MISSED_WINS):\n",
    "                # Add an entry if there was a possible win\n",
    "                possible_win,possible_win_column=check_for_possible_win(gameboard)\n",
    "                if (possible_win) and (reward<RewardForWin):  # Don't bother if we already won, only include possible wins not taken             \n",
    "                    #print('There was a possible win') # deleteme\n",
    "                    #display_gameboard(gameboard) # deleteme\n",
    "                    reward=RewardForWin\n",
    "                    possible_win_row = list(gameboard[possible_win_column]).index(0)\n",
    "                    gameboard_win = gameboard.copy()\n",
    "                    gameboard_win[possible_win_column,possible_win_row] = 1\n",
    "                    observations = np.reshape(gameboard_win, [gameboard_win.shape[0], gameboard_win.shape[1], 1]) \n",
    "                    observation = np.concatenate([observations, observations, observations], axis=2) \n",
    "                    observations_1 = np.reshape(gameboard_win, [gameboard_win.shape[0], gameboard_win.shape[1], 1])\n",
    "                    observation_1 = observation[:, :, 1:] \n",
    "                    observation_1 = np.concatenate([observation_1, observations_1], axis=2) \n",
    "                    episodeBuffer.add(np.reshape(np.array([observation,0,reward,observation_1,1]),[1,5]))\n",
    "                    if (MIRRORED_TRAINING):\n",
    "                        episodeBuffer.add(np.reshape(np.array([-1*observation,0,-reward,-1*observation_1,1]),[1,5])) # Try mirroring\n",
    "                \n",
    "            # Add an entry if there was a possible win for the opponent? \n",
    "            if (TRAIN_MISSED_BLOCKS):\n",
    "                possible_loss,possible_loss_column=check_for_possible_win(-1*gameboard)\n",
    "                if (possible_loss) and (reward<RewardForWin):  # Don't bother if we already won, only include opportunities the opponent could use             \n",
    "                    #print('There was a possible loss') # deleteme\n",
    "                    #display_gameboard(gameboard) # deleteme\n",
    "                    #print('old reward ',reward) # deleteme\n",
    "                    reward= -RewardForWin*possible_loss\n",
    "                    possible_loss_row = list(gameboard[possible_loss_column]).index(0)\n",
    "                    gameboard_loss = gameboard.copy()\n",
    "                    gameboard_loss[possible_loss_column,possible_loss_row] = -1\n",
    "                    #display_gameboard(gameboard_loss) # deleteme\n",
    "                    #print('reward=',reward) # deleteme\n",
    "                    observations = np.reshape(gameboard_loss, [gameboard_loss.shape[0], gameboard_loss.shape[1], 1]) \n",
    "                    observation = np.concatenate([observations, observations, observations], axis=2) \n",
    "                    observations_1 = np.reshape(gameboard_loss, [gameboard_loss.shape[0], gameboard_loss.shape[1], 1])\n",
    "                    observation_1 = observation[:, :, 1:] \n",
    "                    observation_1 = np.concatenate([observation_1, observations_1], axis=2) \n",
    "                    episodeBuffer.add(np.reshape(np.array([observation,0,reward,observation_1,1]),[1,5]))\n",
    "                    if (MIRRORED_TRAINING):\n",
    "                        episodeBuffer.add(np.reshape(np.array([-1*observation,0,-reward,-1*observation_1,1]),[1,5])) # Try mirroring '''\n",
    "                \n",
    "        # Add the accumulated episodes to myBuffer\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        episode_lengths.append(episode_steps)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        # Periodically save the model\n",
    "        if (train_model):\n",
    "          if (game_number % (training_games/100) == 0 and game_number != 0) or (game_number==training_games-1):\n",
    "            saver.save(sess, model_path+'/model-'+str(game_number)+'.cptk')\n",
    "            print(\"Saved Model\")\n",
    "            print (\"Epsilon= %.4f\" %Epsilon,\" Mean Reward: {}\".format(np.mean(episode_rewards[-50:])))\n",
    "            print('Learning Rate = %.6f' %learning_rate)\n",
    "            game_number_list[list_counter] = game_number\n",
    "            mean_reward_list[list_counter] = np.mean(episode_rewards[-50:])\n",
    "            list_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-19 15:15:38.503396\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "import winsound\n",
    "winsound.Beep(500,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(game_number_list, mean_reward_list)\n",
    "\n",
    "plt.title(\"MEAN REWARDS DURING TRAINING\")\n",
    "plt.xlabel(\"Trainging epoch\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "#plt.legend()\n",
    "#plt.savefig('PerformanceVsHsize.png')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(game_number_list)\n",
    "print(mean_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define different players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(gameboard):\n",
    "    #column = random.choice(range(Xdim))\n",
    "    column = random.randint(0,Xdim-1)\n",
    "    while gameboard[column,-1]:\n",
    "       column = random.randint(0,Xdim-1)\n",
    "    return(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent_with_winblocklosssearch_middlestart(gameboard):    \n",
    "        \n",
    "    # On the first move, start in the middle\n",
    "    if (-1) not in gameboard: \n",
    "        return(int(Xdim/2))  \n",
    "    \n",
    "    # Take a win if you have it\n",
    "    possible_win,win_column=check_for_possible_win(gameboard)\n",
    "    if possible_win:\n",
    "        return(win_column)\n",
    "    \n",
    "    # Make a block if you need it\n",
    "    possible_block,block_column=check_for_possible_win(-1*gameboard)\n",
    "    if possible_block: # Make a block if you need it\n",
    "        return(block_column)\n",
    "    \n",
    "    Qstate_value = [0.0 for i in range(Xdim)]\n",
    "    for column in range(Xdim):\n",
    "        if gameboard[column,-1]:\n",
    "            Qstate_value[column]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            Qstate_value[column] = random.random() # Default is random\n",
    "            # Check to see if we have created a winning opportunity for the opponent\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column\n",
    "            if (row<Ydim-1):                        # Have we created a new legal move?\n",
    "                gameboard_possible = gameboard.copy()\n",
    "                gameboard_possible[column,row] = 1\n",
    "                gameboard_possible[column,row+1] = -1\n",
    "                created_loss = check_for_win(-1*gameboard_possible, column, row+1) # Is the opponent's new move a winning move?\n",
    "                if (created_loss):\n",
    "                    Qstate_value[column] = -MAX/2 # Avoid giving them an opportunity\n",
    "                        \n",
    "    column = Qstate_value.index(max(Qstate_value))\n",
    "    return(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Trained agent\n",
    "#\n",
    "tf.reset_default_graph()\n",
    "#mainQN = Qnetwork(h_size, action_space_size, learning_rate, \"main\")\n",
    "#targetQN = Qnetwork(h_size, action_space_size, learning_rate, \"target\")\n",
    "mainQN = Qnetwork(h_size, 1, learning_rate, \"main\")\n",
    "targetQN = Qnetwork(h_size, 1, learning_rate, \"target\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#trainables = tf.trainable_variables()\n",
    "#update_target_ops = update_target_graph(\"main\", \"target\")\n",
    "#myBuffer = experience_buffer()\n",
    "\n",
    "def trained_agent(gameboard):\n",
    "    Qstate_value = [0.0 for i in range(Xdim)]\n",
    "    for column in range(Xdim):\n",
    "        if gameboard[column,-1]:\n",
    "            Qstate_value[column]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column #new\n",
    "            gameboard_possible = gameboard.copy()\n",
    "            gameboard_possible[column,row] = 1\n",
    "  \n",
    "            observations = np.reshape(gameboard_possible, [gameboard_possible.shape[0], gameboard_possible.shape[1], 1]) #new\n",
    "            observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "            Qvalue = sess.run(mainQN.q_out,feed_dict={mainQN.observation_input:[observation]})[0] #new\n",
    "            Qstate_value[column]=Qvalue[0]\n",
    "    \n",
    "    count = Qstate_value.count(max(Qstate_value))\n",
    "    if count>1:\n",
    "        column = random.randint(0,Xdim-1)\n",
    "        while Qstate_value[column] != max(Qstate_value):\n",
    "            column=random.randint(0,Xdim-1)\n",
    "        #print(count,'values tied for the max')\n",
    "    else:\n",
    "        column = Qstate_value.index(max(Qstate_value)) # Find the first/only occurence of the max value\n",
    "    #column = Xdim -1 - Qstate_value[::-1].index(max(Qstate_value))  # Find the last occurance of the max value # deleteme?\n",
    "    return(column)\n",
    "           \n",
    "def trained_agent_verbose(gameboard):\n",
    "    print('\\n')\n",
    "    display_gameboard(gameboard) # deleteme\n",
    "    Qstate_value = [0.0 for i in range(Xdim)]\n",
    "    for column in range(Xdim):\n",
    "        if gameboard[column,-1]:\n",
    "            Qstate_value[column]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column #new\n",
    "            gameboard_possible = gameboard.copy()\n",
    "            gameboard_possible[column,row] = 1\n",
    "  \n",
    "            observations = np.reshape(gameboard_possible, [gameboard_possible.shape[0], gameboard_possible.shape[1], 1]) #new\n",
    "            observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "            Qvalue = sess.run(mainQN.q_out,feed_dict={mainQN.observation_input:[observation]})[0] #new\n",
    "            Qstate_value[column]=Qvalue[0]\n",
    "    \n",
    "    print(Qstate_value) # deleteme\n",
    "    count = Qstate_value.count(max(Qstate_value))\n",
    "    if count>1:\n",
    "        column = random.randint(0,Xdim-1)\n",
    "        while Qstate_value[column] != max(Qstate_value):\n",
    "            column=random.randint(0,Xdim-1)\n",
    "        #print(count,'values tied for the max')\n",
    "    else:\n",
    "        column = Qstate_value.index(max(Qstate_value)) # Find the first occurence of the max value\n",
    "    #column = Xdim -1 - Qstate_value[::-1].index(max(Qstate_value))  # Find the last occurance of the max value # deleteme?\n",
    "    return(column)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_agent_with_winsearch(gameboard):\n",
    "       \n",
    "    # MIDDLESTART: On the first move, start in the middle\n",
    "    #if (-1) not in gameboard: \n",
    "        #return(int(Xdim/2))  \n",
    "    \n",
    "    # WINSEARCH: Take a win if you have it\n",
    "    possible_win,win_column=check_for_possible_win(gameboard)\n",
    "    if possible_win:\n",
    "        return(win_column)\n",
    "    \n",
    "    # BLOCKSEARCH: Make a block if you need it\n",
    "    #possible_block,block_column=check_for_possible_win(-1*gameboard)\n",
    "    #if possible_block: # Make a block if you need it\n",
    "        #return(block_column)\n",
    "       \n",
    "    # Find the best Qvalue among all the options\n",
    "    Qstate_value = [0.0 for i in range(Xdim)]\n",
    "    for column in range(Xdim):\n",
    "        if gameboard[column,-1]:\n",
    "            Qstate_value[column]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column #new\n",
    "            gameboard_possible = gameboard.copy()\n",
    "            gameboard_possible[column,row] = 1\n",
    "            #possible_loss, loss_column = check_for_possible_win(-1*gameboard_possible) # LOSSSEARCH\n",
    "            #if (possible_loss): # avoid giving them an opportunity                     # LOSSSEARCH\n",
    "                #Qstate_value[column] = -MAX/2                                          # LOSSSEARCH\n",
    "            #else:                                                                      # LOSSSEARCH\n",
    "            observations = np.reshape(gameboard_possible, [gameboard_possible.shape[0], gameboard_possible.shape[1], 1]) #new\n",
    "            observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "            Qvalue = sess.run(mainQN.q_out,feed_dict={mainQN.observation_input:[observation]})[0] #new\n",
    "            Qstate_value[column]=Qvalue[0]\n",
    "    \n",
    "    count = Qstate_value.count(max(Qstate_value))\n",
    "    if count>1:\n",
    "        column = random.randint(0,Xdim-1)\n",
    "        while Qstate_value[column] != max(Qstate_value):\n",
    "            column=random.randint(0,Xdim-1)\n",
    "        #print(count,'values tied for the max')\n",
    "    else:\n",
    "        column = Qstate_value.index(max(Qstate_value)) # Find the first/only occurence of the max value\n",
    "    #column = Xdim -1 - Qstate_value[::-1].index(max(Qstate_value))  # Find the last occurance of the max value # deleteme?\n",
    "    return(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_agent_with_winlosssearch_middlestart(gameboard):\n",
    "       \n",
    "    # MIDDLESTART: On the first move, start in the middle\n",
    "    if (-1) not in gameboard: \n",
    "        return(int(Xdim/2))  \n",
    "    \n",
    "    # WINSEARCH: Take a win if you have it\n",
    "    possible_win,win_column=check_for_possible_win(gameboard)\n",
    "    if possible_win:\n",
    "        return(win_column)\n",
    "    \n",
    "    # BLOCKSEARCH: Make a block if you need it\n",
    "    possible_block,block_column=check_for_possible_win(-1*gameboard)\n",
    "    if possible_block: # Make a block if you need it\n",
    "        return(block_column)\n",
    "       \n",
    "    # Find the best Qvalue among all the options\n",
    "    Qstate_value = [0.0 for i in range(Xdim)]\n",
    "    for column in range(Xdim):\n",
    "        if gameboard[column,-1]:\n",
    "            Qstate_value[column]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column #new\n",
    "            gameboard_possible = gameboard.copy()\n",
    "            gameboard_possible[column,row] = 1\n",
    "            possible_loss, loss_column = check_for_possible_win(-1*gameboard_possible) # LOSSSEARCH\n",
    "            if (possible_loss): # avoid giving them an opportunity                     # LOSSSEARCH\n",
    "                Qstate_value[column] = -MAX/2                                          # LOSSSEARCH\n",
    "            else:                                                                      # LOSSSEARCH\n",
    "                observations = np.reshape(gameboard_possible, [gameboard_possible.shape[0], gameboard_possible.shape[1], 1]) #new\n",
    "                observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "                Qvalue = sess.run(mainQN.q_out,feed_dict={mainQN.observation_input:[observation]})[0] #new\n",
    "                Qstate_value[column]=Qvalue[0]\n",
    "    \n",
    "    count = Qstate_value.count(max(Qstate_value))\n",
    "    if count>1:\n",
    "        column = random.randint(0,Xdim-1)\n",
    "        while Qstate_value[column] != max(Qstate_value):\n",
    "            column=random.randint(0,Xdim-1)\n",
    "        #print(count,'values tied for the max')\n",
    "    else:\n",
    "        column = Qstate_value.index(max(Qstate_value)) # Find the first/only occurence of the max value\n",
    "    #column = Xdim -1 - Qstate_value[::-1].index(max(Qstate_value))  # Find the last occurance of the max value # deleteme?\n",
    "    return(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_agent_with_middlestart(gameboard):\n",
    "           \n",
    "    # MIDDLESTART: On the first move, start in the middle\n",
    "    if (-1) not in gameboard: \n",
    "        return(int(Xdim/2))  \n",
    "    \n",
    "     # WINSEARCH: Take a win if you have it\n",
    "    #possible_win,win_column=check_for_possible_win(gameboard)\n",
    "    #if possible_win:\n",
    "        #return(win_column)\n",
    "    \n",
    "    # BLOCKSEARCH: Make a block if you need it\n",
    "    #possible_block,block_column=check_for_possible_win(-1*gameboard)\n",
    "    #if possible_block: # Make a block if you need it\n",
    "        #return(block_column)\n",
    "       \n",
    "    # Find the best Qvalue among all the options\n",
    "    Qstate_value = [0.0 for i in range(Xdim)]\n",
    "    for column in range(Xdim):\n",
    "        if gameboard[column,-1]:\n",
    "            Qstate_value[column]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column #new\n",
    "            gameboard_possible = gameboard.copy()\n",
    "            gameboard_possible[column,row] = 1\n",
    "            #possible_loss, loss_column = check_for_possible_win(-1*gameboard_possible) # LOSSSEARCH\n",
    "            #if (possible_loss): # avoid giving them an opportunity                     # LOSSSEARCH\n",
    "                #Qstate_value[column] = -MAX/2                                          # LOSSSEARCH\n",
    "            #else:                                                                      # LOSSSEARCH\n",
    "            observations = np.reshape(gameboard_possible, [gameboard_possible.shape[0], gameboard_possible.shape[1], 1]) #new\n",
    "            observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "            Qvalue = sess.run(mainQN.q_out,feed_dict={mainQN.observation_input:[observation]})[0] #new\n",
    "            Qstate_value[column]=Qvalue[0]\n",
    "    \n",
    "    count = Qstate_value.count(max(Qstate_value))\n",
    "    if count>1:\n",
    "        column = random.randint(0,Xdim-1)\n",
    "        while Qstate_value[column] != max(Qstate_value):\n",
    "            column=random.randint(0,Xdim-1)\n",
    "        #print(count,'values tied for the max')\n",
    "    else:\n",
    "        column = Qstate_value.index(max(Qstate_value)) # Find the first/only occurence of the max value\n",
    "    #column = Xdim -1 - Qstate_value[::-1].index(max(Qstate_value))  # Find the last occurance of the max value # deleteme?\n",
    "    return(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent_with_winlosssearch(gameboard):\n",
    "    \n",
    "    # Take a win if you have it\n",
    "    possible_win,win_column=check_for_possible_win(gameboard)\n",
    "    if possible_win:\n",
    "        return(win_column)\n",
    "    \n",
    "    # Make a block if you need it\n",
    "    possible_block,block_column=check_for_possible_win(-1*gameboard)\n",
    "    if possible_block: # Make a block if you need it\n",
    "        return(block_column)\n",
    "    \n",
    "    Qstate_value = [0.0 for i in range(Xdim)]\n",
    "    for column in range(Xdim):\n",
    "        if gameboard[column,-1]:\n",
    "            Qstate_value[column]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column #new\n",
    "            gameboard_possible = gameboard.copy()\n",
    "            gameboard_possible[column,row] = 1\n",
    "            possible_loss, loss_column = check_for_possible_win(-1*gameboard_possible)\n",
    "            if (possible_loss): # avoid giving them an opportunity\n",
    "                Qstate_value[column] = -MAX/2\n",
    "            else:\n",
    "                # Default is random\n",
    "                Qstate_value[column]=random.random()\n",
    "                        \n",
    "    column = Qstate_value.index(max(Qstate_value))\n",
    "    return(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent_with_winlosssearch_middlestart(gameboard):    \n",
    "        \n",
    "    # On the first move, start in the middle\n",
    "    if (-1) not in gameboard: \n",
    "        return(int(Xdim/2))  \n",
    "    \n",
    "    # Take a win if you have it\n",
    "    possible_win,win_column=check_for_possible_win(gameboard)\n",
    "    if possible_win:\n",
    "        return(win_column)\n",
    "    \n",
    "    # Make a block if you need it\n",
    "    possible_block,block_column=check_for_possible_win(-1*gameboard)\n",
    "    if possible_block: # Make a block if you need it\n",
    "        return(block_column)\n",
    "    \n",
    "    Qstate_value = [0.0 for i in range(Xdim)]\n",
    "    for column in range(Xdim):\n",
    "        if gameboard[column,-1]:\n",
    "            Qstate_value[column]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column #new\n",
    "            gameboard_possible = gameboard.copy()\n",
    "            gameboard_possible[column,row] = 1\n",
    "            possible_loss, loss_column = check_for_possible_win(-1*gameboard_possible)\n",
    "            if (possible_loss): # avoid giving them an opportunity\n",
    "                Qstate_value[column] = -MAX/2\n",
    "            else:\n",
    "                # Default is random\n",
    "                Qstate_value[column]=random.random()\n",
    "                        \n",
    "    column = Qstate_value.index(max(Qstate_value))\n",
    "    return(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this routine every move to evaluate what the player accomplished or missed\n",
    "#\n",
    "def update_stats(gameboard,column,row,episode_steps,victories,possible_wins,missed_wins,possible_blocks,missed_blocks, \\\n",
    "                 created_possible_losses, middle_starts):\n",
    "    global player1_victories\n",
    "    global player2_victories\n",
    "    done=0\n",
    "    \n",
    "    #Did we start in the middle?\n",
    "    if (episode_steps==1) and (column==int(Xdim/2)):\n",
    "        middle_starts+=1                 \n",
    "    \n",
    "    # Did we win?\n",
    "    if check_for_win(gameboard,column,row):\n",
    "        victories+=1\n",
    "        possible_wins+=1\n",
    "        if (episode_steps % 2):\n",
    "            player1_victories+=1\n",
    "        else:\n",
    "            player2_victories+=1\n",
    "        done=1\n",
    "        return(done,victories,possible_wins,missed_wins,possible_blocks,missed_blocks,created_possible_losses,middle_starts)\n",
    "        \n",
    "    # Did we miss a possible win?\n",
    "    gameboard_before_move = gameboard.copy()\n",
    "    gameboard_before_move[column,row]=0\n",
    "    possible_win,win_column=check_for_possible_win(gameboard_before_move)\n",
    "    if possible_win:\n",
    "        possible_wins+=1\n",
    "        missed_wins+=1\n",
    "        \n",
    "    # Are we making a block?\n",
    "    gameboard_opponent = -1*gameboard\n",
    "    gameboard_opponent[column,row]=1\n",
    "    if check_for_win(gameboard_opponent,column,row): # see if we are making a block\n",
    "        possible_blocks+=1\n",
    "        return(done,victories,possible_wins,missed_wins,possible_blocks,missed_blocks,created_possible_losses,middle_starts)\n",
    "        \n",
    "    #Is there a non-losing move?\n",
    "    non_losing_move = [1 for i in range(Xdim)]\n",
    "    for possible_column in range(Xdim):\n",
    "        if gameboard_before_move[possible_column,-1]: # Rule out this column if it's full\n",
    "            non_losing_move[possible_column]=0\n",
    "            continue\n",
    "        possible_row = list(gameboard_before_move[possible_column]).index(0) # Find the lowest open slot in the chosen column \n",
    "        gameboard_possible = gameboard_before_move.copy()\n",
    "        gameboard_possible[possible_column,possible_row] = 1\n",
    "        possible_loss, loss_column = check_for_possible_win(-1*gameboard_possible)\n",
    "        if possible_loss:\n",
    "            non_losing_move[possible_column]=0 # Rule out this column if it leads to a possible loss\n",
    "    if 1 not in non_losing_move:\n",
    "        return(done,victories,possible_wins,missed_wins,possible_blocks,missed_blocks,created_possible_losses,middle_starts)\n",
    "              \n",
    "    #Did we miss a possible block?\n",
    "    gameboard_opponent = -1*gameboard\n",
    "    possible_loss,loss_column=check_for_possible_win(gameboard_opponent) # see if the opponent can win next\n",
    "    gameboard_opponent[column,row]= 0\n",
    "    possible_block,block_column=check_for_possible_win(gameboard_opponent) # see if there was a block to be made\n",
    "    if (possible_block):  \n",
    "        possible_blocks+=1\n",
    "        missed_blocks+=1\n",
    "    else:\n",
    "        if (possible_loss): # we have created an opportunity for our opponent\n",
    "            created_possible_losses+=1\n",
    "            \n",
    "    done=0\n",
    "    return(done,victories,possible_wins,missed_wins,possible_blocks,missed_blocks,created_possible_losses,middle_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this routine every move to evaluate what the player accomplished or missed\n",
    "#\n",
    "def update_stats_verbose(gameboard,column,row,episode_steps,victories,possible_wins,missed_wins,possible_blocks,missed_blocks, \\\n",
    "                 created_possible_losses, middle_starts):\n",
    "    global player1_victories\n",
    "    global player2_victories\n",
    "    done=0\n",
    "    \n",
    "    #Did we start in the middle?\n",
    "    if (episode_steps==1) and (column==int(Xdim/2)):\n",
    "        middle_starts+=1                 \n",
    "    \n",
    "    # Did we win?\n",
    "    if check_for_win(gameboard,column,row):\n",
    "        victories+=1\n",
    "        possible_wins+=1\n",
    "        if (episode_steps % 2):\n",
    "            player1_victories+=1\n",
    "        else:\n",
    "            player2_victories+=1\n",
    "        done=1\n",
    "        return(done,victories,possible_wins,missed_wins,possible_blocks,missed_blocks,created_possible_losses,middle_starts)\n",
    "        \n",
    "    # Did we miss a possible win?\n",
    "    gameboard_before_move = gameboard.copy()\n",
    "    gameboard_before_move[column,row]=0\n",
    "    possible_win,win_column=check_for_possible_win(gameboard_before_move)\n",
    "    if possible_win:\n",
    "        possible_wins+=1\n",
    "        missed_wins+=1\n",
    "        print('MISSED WIN')\n",
    "        print('\\nBefore move')\n",
    "        display_gameboard(gameboard_before_move)\n",
    "        display_gameboard(gameboard)\n",
    "        print('column =',column)\n",
    "        \n",
    "    # Are we making a block?\n",
    "    gameboard_opponent = -1*gameboard\n",
    "    gameboard_opponent[column,row]=1\n",
    "    if check_for_win(gameboard_opponent,column,row): # see if we are making a block\n",
    "        possible_blocks+=1\n",
    "        return(done,victories,possible_wins,missed_wins,possible_blocks,missed_blocks,created_possible_losses,middle_starts)\n",
    "        \n",
    "    #Is there a non-losing move?\n",
    "    non_losing_move = [1 for i in range(Xdim)]\n",
    "    for possible_column in range(Xdim):\n",
    "        if gameboard_before_move[possible_column,-1]: # Rule out this column if it's full\n",
    "            non_losing_move[possible_column]=0\n",
    "            continue\n",
    "        possible_row = list(gameboard_before_move[possible_column]).index(0) # Find the lowest open slot in the chosen column \n",
    "        gameboard_possible = gameboard_before_move.copy()\n",
    "        gameboard_possible[possible_column,possible_row] = 1\n",
    "        possible_loss, loss_column = check_for_possible_win(-1*gameboard_possible)\n",
    "        if possible_loss:\n",
    "            non_losing_move[possible_column]=0 # Rule out this column if it leads to a possible loss\n",
    "    if 1 not in non_losing_move:\n",
    "        return(done,victories,possible_wins,missed_wins,possible_blocks,missed_blocks,created_possible_losses,middle_starts)\n",
    "              \n",
    "    #Did we miss a possible block?\n",
    "    gameboard_opponent = -1*gameboard\n",
    "    possible_loss,loss_column=check_for_possible_win(gameboard_opponent) # see if the opponent can win next\n",
    "    gameboard_opponent[column,row]= 0\n",
    "    possible_block,block_column=check_for_possible_win(gameboard_opponent) # see if there was a block to be made\n",
    "    if (possible_block):  \n",
    "        possible_blocks+=1\n",
    "        missed_blocks+=1\n",
    "        print('MISSED BLOCK')\n",
    "        print('\\nBefore move')\n",
    "        display_gameboard(gameboard_before_move)\n",
    "        display_gameboard(gameboard)\n",
    "        print('column =',column)\n",
    "    else:\n",
    "        if (possible_loss): # we have created an opportunity for our opponent\n",
    "            created_possible_losses+=1\n",
    "            print('CREATED LOSS')\n",
    "            print('\\nBefore move')\n",
    "            display_gameboard(gameboard_before_move)\n",
    "            display_gameboard(gameboard)\n",
    "            print('column =',column)\n",
    "            \n",
    "    done=0\n",
    "    return(done,victories,possible_wins,missed_wins,possible_blocks,missed_blocks,created_possible_losses,middle_starts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play two opponents against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerA = trained_agent\n",
    "playerB = random_agent_with_winblocklosssearch_middlestart\n",
    "test_games= 10000\n",
    "test_games= 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-19 15:15:40.670588\n"
     ]
    }
   ],
   "source": [
    " print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Trainings_500Ktraining/models4/dqn_training_rewardEveryMove-Copy\\model-499999.cptk\n",
      "Model loaded\n",
      "\n",
      "PlayerA is a <function trained_agent at 0x000001F5B0A74B70>\n",
      "PlayerB is a <function random_agent_with_winblocklosssearch_middlestart at 0x000001F5A3A8FBF8>\n",
      "\n",
      "PlayerA won 4380 games for 87.6%\n",
      "PlayerA went first 49.94% of the time and won 2377 of those games for 95.2%\n",
      "PlayerA started in the middle 2497 times out of 2497 (100.0%)\n",
      "PlayerA won 4380 times out of 4444 chances (98.6%)\n",
      "PlayerA blocked 4085 times out of 4141 chances (98.6%)\n",
      "PlayerA created 18 opportunities for PlayerB\n",
      "PlayerA distribution = [5040, 9194, 9842, 12349, 11152, 5245, 3695]\n",
      "\n",
      "PlayerB won 443 games for 8.86%\n",
      "PlayerB went first 50.06% of the time and won 358 of those games for 14.3%\n",
      "PlayerB started in the middle 2503 times out of 2503 (100.0%)\n",
      "PlayerB won 443 times out of 443 chances (100.0%)\n",
      "PlayerB blocked 12495 times out of 12495 chances (100.0%)\n",
      "PlayerB created 0 opportunities for PlayerA\n",
      "PlayerB distribution = [7251, 7294, 7142, 9255, 7794, 7806, 7956]\n",
      "\n",
      "Player1 won 2735 games for 54.7%\n",
      "Player2 won 2088 games for 41.76%\n",
      "177 tied games for 3.54%\n"
     ]
    }
   ],
   "source": [
    "#This is up-to-date altered\n",
    "#\n",
    "# Play a set of matches\n",
    "#\n",
    "global player1_victories\n",
    "global player2_victories\n",
    "player1_victories = 0\n",
    "player2_victories = 0\n",
    "playerA_victories = 0\n",
    "playerB_victories = 0\n",
    "playerA_possible_wins = 0\n",
    "playerB_possible_wins = 0\n",
    "playerA_missed_wins = 0\n",
    "playerB_missed_wins = 0\n",
    "playerA_possible_blocks = 0\n",
    "playerB_possible_blocks = 0\n",
    "playerA_missed_blocks = 0\n",
    "playerB_missed_blocks = 0\n",
    "playerA_created_possible_losses = 0\n",
    "playerB_created_possible_losses = 0\n",
    "playerA_starts = 0\n",
    "playerB_starts = 0\n",
    "playerA_middle_starts = 0\n",
    "playerB_middle_starts = 0\n",
    "playerA_firstplayerwins = 0\n",
    "playerB_firstplayerwins = 0\n",
    "tied_games=0\n",
    "gameboard_history = np.zeros((test_games,Xdim,Ydim),dtype=int) # use this to use gameboard_history to record all final gameboards\n",
    "playerA_distribution = [0]*Xdim\n",
    "playerB_distribution = [0]*Xdim\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "  saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "  print('Model loaded')\n",
    "    \n",
    "  for game in range(test_games):   \n",
    "    # reset gameboard\n",
    "    gameboard = np.zeros((Xdim,Ydim),dtype=int) \n",
    "    #gameboard_history = np.zeros((Xdim*Ydim,Xdim,Ydim),dtype=int) # Use this to use gameboard_history to record steps in a game\n",
    "    observations = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) #new\n",
    "    observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "    done = False\n",
    "       \n",
    "    playerA_first = (random.random()<0.5)\n",
    "    if playerA_first:\n",
    "        playerA_starts+=1\n",
    "    else:\n",
    "        playerB_starts+=1\n",
    "    episode_steps = 0    \n",
    "    while not done and 0 in gameboard:\n",
    "        \n",
    "        #Player A\n",
    "        if (episode_steps>0) or playerA_first:\n",
    "            gameboard *=-1 # switch sides every turn \n",
    "            episode_steps+=1\n",
    "            # Determine our move\n",
    "            column=playerA(gameboard)\n",
    "            playerA_distribution[column]+=1 # deleteme\n",
    "            #display_gameboard(gameboard) # deleteme\n",
    "            #print('trained', column) # deleteme\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column \n",
    "            gameboard[column,row] = 1 \n",
    "            # Update our stats\n",
    "            done, playerA_victories, playerA_possible_wins, playerA_missed_wins, \\\n",
    "                playerA_possible_blocks, playerA_missed_blocks, playerA_created_possible_losses, playerA_middle_starts\\\n",
    "                = update_stats(gameboard,column, row,episode_steps, playerA_victories, playerA_possible_wins, playerA_missed_wins, \\\n",
    "                playerA_possible_blocks, playerA_missed_blocks, playerA_created_possible_losses, playerA_middle_starts)\n",
    "            if done:\n",
    "                gameboard_history[game] = gameboard\n",
    "                if playerA_first:\n",
    "                    playerA_firstplayerwins +=1\n",
    "                break # PlayerA won!\n",
    "        \n",
    "        # Check for a tie\n",
    "        if (0 not in gameboard):\n",
    "            tied_games+=1\n",
    "            gameboard_history[game] = gameboard\n",
    "            break\n",
    "            \n",
    "        #PlayerB        \n",
    "        gameboard *=-1 # switch sides every turn \n",
    "        episode_steps+=1\n",
    "        # Determine our move\n",
    "        #display_gameboard(gameboard) # deleteme\n",
    "        column=playerB(gameboard)\n",
    "        playerB_distribution[column]+=1 # deleteme\n",
    "        row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column \n",
    "        gameboard[column,row] = 1 \n",
    "        # Update our stats\n",
    "        done, playerB_victories, playerB_possible_wins, playerB_missed_wins, \\\n",
    "            playerB_possible_blocks, playerB_missed_blocks, playerB_created_possible_losses, playerB_middle_starts\\\n",
    "            = update_stats(gameboard,column, row,episode_steps, playerB_victories, playerB_possible_wins, playerB_missed_wins, \\\n",
    "            playerB_possible_blocks, playerB_missed_blocks, playerB_created_possible_losses, playerB_middle_starts)\n",
    "        if done:\n",
    "            gameboard_history[game] = gameboard\n",
    "            if not playerA_first:\n",
    "                playerB_firstplayerwins +=1\n",
    "            #print('playerB wins!')  # deleteme\n",
    "            break # PlayerB won!\n",
    "            \n",
    "        # Check for a tie\n",
    "        if (0 not in gameboard):\n",
    "            tied_games+=1\n",
    "            gameboard_history[game] = gameboard\n",
    "            break\n",
    "     \n",
    "    if (TRAIN_AFTER_TESTGAMES):\n",
    "        # At the end of each game, Get a random batch of experiences.\n",
    "        trainBatch = myBuffer.sample(batch_size) \n",
    "        # Below we perform the Double-DQN update to the target Q-values\n",
    "        Q1 = sess.run(mainQN.predict, feed_dict={mainQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "        Q2 = sess.run(targetQN.q_out, feed_dict={targetQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "        end_multiplier = -(trainBatch[:,4] - 1)\n",
    "        doubleQ = Q2[range(batch_size),Q1]\n",
    "        targetQ = trainBatch[:,2] + (targetQ_y*doubleQ * end_multiplier)\n",
    "        # Update the network with our target values.\n",
    "        _, q_loss = sess.run([mainQN.update, mainQN.loss],\n",
    "            feed_dict={mainQN.observation_input:np.stack(trainBatch[:,0], axis=0),\n",
    "                    mainQN.targetQ:targetQ, \n",
    "                    mainQN.actions:trainBatch[:,1]})\n",
    "        losses.append(q_loss)\n",
    "\n",
    "print('\\nPlayerA is a',playerA)\n",
    "print('PlayerB is a',playerB)\n",
    "print(f'\\nPlayerA won {playerA_victories} games for {100*playerA_victories/test_games}%')\n",
    "print(f'PlayerA went first {100*playerA_starts/test_games}% of the time and won {playerA_firstplayerwins} of those games for {round(10*100*playerA_firstplayerwins/max(1,playerA_starts))/10}%')\n",
    "print(f'PlayerA started in the middle {playerA_middle_starts} times out of {playerA_starts} ({round(10*100*playerA_middle_starts/max(1,playerA_starts))/10}%)')\n",
    "print(f'PlayerA won {playerA_victories} times out of {playerA_possible_wins} chances ({round(10*100*playerA_victories/max(1,playerA_possible_wins))/10}%)')\n",
    "print(f'PlayerA blocked {playerA_possible_blocks-playerA_missed_blocks} times out of {playerA_possible_blocks} chances ({round(10*100*(1-playerA_missed_blocks/max(1,playerA_possible_blocks)))/10}%)')\n",
    "print(f'PlayerA created {playerA_created_possible_losses} opportunities for PlayerB')\n",
    "print(f'PlayerA distribution = {playerA_distribution}')\n",
    "print(f'\\nPlayerB won {playerB_victories} games for {100*playerB_victories/test_games}%')\n",
    "print(f'PlayerB went first {100*playerB_starts/test_games}% of the time and won {playerB_firstplayerwins} of those games for {round(10*100*playerB_firstplayerwins/max(1,playerB_starts))/10}%')\n",
    "print(f'PlayerB started in the middle {playerB_middle_starts} times out of {playerB_starts} ({round(10*100*playerB_middle_starts/max(1,playerB_starts))/10}%)')\n",
    "print(f'PlayerB won {playerB_victories} times out of {playerB_possible_wins} chances ({round(10*100*playerB_victories/max(1,playerB_possible_wins))/10}%)')\n",
    "print(f'PlayerB blocked {playerB_possible_blocks-playerB_missed_blocks} times out of {playerB_possible_blocks} chances ({round(10*100*(1-playerB_missed_blocks/max(1,playerB_possible_blocks)))/10}%)')\n",
    "print(f'PlayerB created {playerB_created_possible_losses} opportunities for PlayerA')\n",
    "print(f'PlayerB distribution = {playerB_distribution}')\n",
    "print(f'\\nPlayer1 won {player1_victories} games for {100*player1_victories/test_games}%')\n",
    "print(f'Player2 won {player2_victories} games for {100*player2_victories/test_games}%')\n",
    "print(f'{tied_games} tied games for {100*tied_games/test_games}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size= 64\n",
      "update_freq =  8\n",
      "targetQ_y= 0.4\n",
      "training_games = 1\n",
      "annealing_time = 0.6\n",
      "number of units in hidden layer =  512\n",
      "learning_rate=  0.0003\n",
      "Mean Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('batch_size=', batch_size)\n",
    "print('update_freq = ',update_freq)\n",
    "print('targetQ_y=',targetQ_y)\n",
    "print('training_games =',training_games)\n",
    "print('annealing_time =', annealing_time)\n",
    "print('number of units in hidden layer = ',h_size)\n",
    "print('learning_rate= ',learning_rate)\n",
    "print (\"Mean Reward: {}\".format(np.mean(episode_rewards[-50:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-19 15:23:31.111585\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT: STARTING INTHE MIDDLE INCREASES FIRST-PLAYER-WINS BY 7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT: ADDING THE EXTRA TRAINING FOR BLOCKING DECREASED ALL THE RESULTS THE FIRST TIME.\n",
    "# RE-RAN, AND THE FIRST-PLAYER-WINS IS UP, BUT BLOCKING IS DOWN. (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT: ADDING THE TRAINING FOR MISSED WIN-OPPORTUNITIES INCREASES THE WIN-CONVERSATION (94.5%),\n",
    "# BUT THE OVERALL WIN RATES WERE DOWN\n",
    "# FORCING A MIDDLE-START FIXES THAT PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result: Increased training still improves results up to 450,000 games, it increases the first-player-wins by 6%, increases starting in the middle from 0% to 100%, increases capitalizing-win-possibilities by 4%, blocking percent +5.7% and creates 30% fewer opportunities for the opponent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 500  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Qvalue(gameboard):\n",
    "    observations = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1])\n",
    "    observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "    Qvalue = sess.run(mainQN.q_out,feed_dict={mainQN.observation_input:[observation]})[0]\n",
    "    return(Qvalue[0])\n",
    "\n",
    "def trained_agent_minimax(gameboard):\n",
    "    Qstate_valueA = [0.0 for i in range(Xdim)]\n",
    "    Qstate_valueB = [0.0 for i in range(Xdim)]\n",
    "    \n",
    "    for columnA in range(Xdim):\n",
    "        gameboardA = gameboard.copy()\n",
    "        if gameboardA[columnA,-1]:\n",
    "            Qstate_valueA[columnA]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboardA[columnA]).index(0) # Find the lowest open slot in the chosen column\n",
    "            gameboardA[columnA,row] = 1\n",
    "            if (check_for_win(gameboardA,columnA,row)):\n",
    "                Qstate_valueA[columnA] = get_Qvalue(gameboardA)\n",
    "                #Qstate_valueA[columnA] = MAX/2\n",
    "            else:\n",
    "                    \n",
    "                for columnB in range(Xdim):\n",
    "                    gameboardB = -1*gameboardA.copy()\n",
    "                    if gameboardB[columnB,-1]:\n",
    "                        Qstate_valueB[columnB] = -MAX # don't allow an illegal move into a full column\n",
    "                    else:\n",
    "                        row = list(gameboardB[columnB]).index(0) # Find the lowest open slot in the chosen column\n",
    "                        gameboardB[columnB,row] = 1                   \n",
    "                        Qstate_valueB[columnB]= get_Qvalue(gameboardB)\n",
    "                       \n",
    "                columnBmax = Qstate_valueB.index(max(Qstate_valueB)) #opponent maximizes their Qvalue for each option\n",
    "                Qstate_valueA[columnA] = -1*max(Qstate_valueB) # Invert the opponent's gain to be our loss\n",
    "    \n",
    "    count = Qstate_valueA.count(max(Qstate_valueA))\n",
    "    if count>1:\n",
    "        columnA = random.randint(0,Xdim-1)\n",
    "        while Qstate_valueA[columnA] != max(Qstate_valueA):\n",
    "            columnA=random.randint(0,Xdim-1)\n",
    "        #print(count,'values tied for the max')\n",
    "    else:\n",
    "        columnA = Qstate_valueA.index(max(Qstate_valueA)) # Find the first/only occurence of the max value\n",
    "    \n",
    "    if gameboard[columnA,-1]:\n",
    "        display_gameboard(gameboard)\n",
    "        print('column =', column)\n",
    "        print(Qstate_value)\n",
    "        print('Something has gone wrong!!')\n",
    "    return(columnA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_agent_minimax_lookahead1(gameboard):\n",
    "    \n",
    "    Qstate_valueA = [0.0 for i in range(Xdim)]\n",
    "    for columnA in range(Xdim):\n",
    "        gameboardA = gameboard.copy()\n",
    "        if gameboardA[columnA,-1]:\n",
    "            Qstate_valueA[columnA]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboardA[columnA]).index(0) # Find the lowest open slot in the chosen column\n",
    "            gameboardA[columnA,row] = 1\n",
    "            if (check_for_win(gameboardA,columnA,row)) or (0 not in gameboardA):\n",
    "                Qstate_valueA[columnA] = get_Qvalue(gameboardA)\n",
    "                #if (check_for_win(gameboardA,columnA,row)):\n",
    "                    #Qstate_valueA[columnA] = MAX/2\n",
    "                #break # No need to evaluate deeper if this terminates the game\n",
    "            else:\n",
    "                 \n",
    "                Qstate_valueB = [0.0 for i in range(Xdim)]    \n",
    "                for columnB in range(Xdim):\n",
    "                    gameboardB = -1*gameboardA.copy()\n",
    "                    if gameboardB[columnB,-1]:\n",
    "                        Qstate_valueB[columnB] = -MAX # don't allow an illegal move into a full column\n",
    "                    else:\n",
    "                        row = list(gameboardB[columnB]).index(0) # Find the lowest open slot in the chosen column\n",
    "                        gameboardB[columnB,row] = 1                   \n",
    "                        #Qstate_valueB[columnB]= get_Qvalue(gameboardB)\n",
    "                        if (check_for_win(gameboardB,columnB,row)) or (0 not in gameboardB):\n",
    "                            Qstate_valueB[columnB] = get_Qvalue(gameboardB)\n",
    "                            #Qstate_valueB[columnB] = MAX/3\n",
    "                        else:\n",
    "                            Qstate_valueB[columnB] = get_Qvalue(gameboardB)\n",
    "                \n",
    "                # This runs at the end of the columnB loop\n",
    "                #columnBmax = Qstate_valueB.index(max(Qstate_valueB)) #opponent maximizes their Qvalue for each option\n",
    "                Qstate_valueA[columnA] = -1*max(Qstate_valueB) # Invert the opponent's gain to be our loss\n",
    "   \n",
    "    count = Qstate_valueA.count(max(Qstate_valueA))\n",
    "    if count>1:\n",
    "        columnA = random.randint(0,Xdim-1)\n",
    "        while Qstate_valueA[columnA] != max(Qstate_valueA):\n",
    "            columnA=random.randint(0,Xdim-1)\n",
    "    else:\n",
    "        columnA = Qstate_valueA.index(max(Qstate_valueA)) # Find the first/only occurence of the max value\n",
    "    \n",
    "    return(columnA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_agent_minimax_lookahead1_pruned(gameboard):\n",
    "    \n",
    "    thresholdA = MAX\n",
    "    Qstate_valueA = [0.0 for i in range(Xdim)]\n",
    "    for columnA in range(Xdim):\n",
    "        gameboardA = gameboard.copy()\n",
    "        if gameboardA[columnA,-1]:\n",
    "            Qstate_valueA[columnA]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboardA[columnA]).index(0) # Find the lowest open slot in the chosen column\n",
    "            gameboardA[columnA,row] = 1\n",
    "            if (check_for_win(gameboardA,columnA,row)) or (0 not in gameboardA):\n",
    "                Qstate_valueA[columnA] = get_Qvalue(gameboardA)\n",
    "                #if (check_for_win(gameboardA,columnA,row)):\n",
    "                    #Qstate_valueA[columnA] = MAX/2\n",
    "                #break # No need to evaluate deeper if this terminates the game\n",
    "            else:\n",
    "                \n",
    "                Qstate_valueB = [0.0 for i in range(Xdim)]\n",
    "                for columnB in range(Xdim):\n",
    "                    gameboardB = -1*gameboardA.copy()\n",
    "                    if gameboardB[columnB,-1]:\n",
    "                        Qstate_valueB[columnB] = -MAX # don't allow an illegal move into a full column\n",
    "                    else:\n",
    "                        row = list(gameboardB[columnB]).index(0) # Find the lowest open slot in the chosen column\n",
    "                        gameboardB[columnB,row] = 1                   \n",
    "                        #Qstate_valueB[columnB]= get_Qvalue(gameboardB)\n",
    "                        if (check_for_win(gameboardB,columnB,row)):\n",
    "                            Qstate_valueB[columnB] = get_Qvalue(gameboardB)\n",
    "                            #Qstate_valueB[columnB] = MAX/3\n",
    "                        else:\n",
    "                            Qstate_valueB[columnB] = get_Qvalue(gameboardB)\n",
    "                    if (Qstate_valueB[columnB] > thresholdA): # If even one of B's options is higher, we don't need to evaluate further\n",
    "                        break #PRUNING                           \n",
    "                \n",
    "                # This runs at the end of the columnB loop\n",
    "                #columnBmax = Qstate_valueB.index(max(Qstate_valueB)) #opponent maximizes their Qvalue for each option\n",
    "                Qstate_valueB_max = max(Qstate_valueB)\n",
    "                if (Qstate_valueB_max<thresholdA):\n",
    "                    thresholdA = Qstate_valueB_max\n",
    "                Qstate_valueA[columnA] = -1*Qstate_valueB_max # Invert the opponent's gain to be our loss\n",
    "   \n",
    "    count = Qstate_valueA.count(max(Qstate_valueA))\n",
    "    if count>1:\n",
    "        columnA = random.randint(0,Xdim-1)\n",
    "        while Qstate_valueA[columnA] != max(Qstate_valueA):\n",
    "            columnA=random.randint(0,Xdim-1)\n",
    "    else:\n",
    "        columnA = Qstate_valueA.index(max(Qstate_valueA)) # Find the first/only occurence of the max value\n",
    "    \n",
    "    return(columnA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_agent_minimax_lookahead2(gameboard):\n",
    " \n",
    "    Qstate_valueA = [0.0 for i in range(Xdim)]    \n",
    "    for columnA in range(Xdim):\n",
    "        gameboardA = gameboard.copy()\n",
    "        if gameboardA[columnA,-1]:\n",
    "            Qstate_valueA[columnA]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboardA[columnA]).index(0) # Find the lowest open slot in the chosen column\n",
    "            gameboardA[columnA,row] = 1\n",
    "            if (check_for_win(gameboardA,columnA,row)) or (0 not in gameboardA):\n",
    "                Qstate_valueA[columnA] = get_Qvalue(gameboardA)\n",
    "                #if (check_for_win(gameboardA,columnA,row)):\n",
    "                    #Qstate_valueA[columnA] = MAX/2\n",
    "                #break # No need to evaluate deeper if this terminates the game\n",
    "            else:\n",
    "                \n",
    "                Qstate_valueB = [0.0 for i in range(Xdim)]\n",
    "                for columnB in range(Xdim):\n",
    "                    gameboardB = -1*gameboardA.copy()\n",
    "                    if gameboardB[columnB,-1]:\n",
    "                        Qstate_valueB[columnB] = -MAX # don't allow an illegal move into a full column\n",
    "                    else:\n",
    "                        row = list(gameboardB[columnB]).index(0) # Find the lowest open slot in the chosen column\n",
    "                        gameboardB[columnB,row] = 1\n",
    "                        if (check_for_win(gameboardB,columnB,row)):\n",
    "                            Qstate_valueB[columnB] = get_Qvalue(gameboardB)\n",
    "                            #Qstate_valueB[columnB] = MAX/3\n",
    "                            break # No need to evaluate deeper if this terminates the game\n",
    "                        if (0 not in gameboardB):\n",
    "                            Qstate_valueB[columnB] = get_Qvalue(gameboardB)\n",
    "                            break # No need to evaluate deeper if this fills the board\n",
    "                        else:\n",
    "                        \n",
    "                            Qstate_valueC = [0.0 for i in range(Xdim)]\n",
    "                            for columnC in range(Xdim):\n",
    "                                gameboardC = -1*gameboardB.copy()\n",
    "                                if gameboardC[columnC,-1]:\n",
    "                                    Qstate_valueC[columnC] = -MAX # don't allow an illegal move into a full column\n",
    "                                else:\n",
    "                                    row = list(gameboardC[columnC]).index(0) # Find the lowest open slot in the chosen column\n",
    "                                    gameboardC[columnC,row] = 1\n",
    "                                    if (check_for_win(gameboardC,columnC,row)):\n",
    "                                        Qstate_valueC[columnC] = get_Qvalue(gameboardC)\n",
    "                                        #Qstate_valueC[columnC] = MAX/4\n",
    "                                    else:\n",
    "                                        Qstate_valueC[columnC]= get_Qvalue(gameboardC)\n",
    "                            # This runs at the end of the columnC loop\n",
    "                            #columnCmax = Qstate_valueB.index(max(Qstate_valueC))\n",
    "                            Qstate_valueB[columnB] = -1*max(Qstate_valueC)\n",
    "                            \n",
    "                # This runs at the end of the columnB loop of all values of columnB            \n",
    "                #columnBmax = Qstate_valueB.index(max(Qstate_valueB)) #opponent maximizes their Qvalue for each option\n",
    "                Qstate_valueA[columnA] = -1*max(Qstate_valueB) # Invert the opponent's gain to be our loss\n",
    "\n",
    "    count = Qstate_valueA.count(max(Qstate_valueA))\n",
    "    if count>1:\n",
    "        columnA = random.randint(0,Xdim-1)\n",
    "        while Qstate_valueA[columnA] != max(Qstate_valueA):\n",
    "            columnA=random.randint(0,Xdim-1)\n",
    "    else:\n",
    "        columnA = Qstate_valueA.index(max(Qstate_valueA)) # Find the first/only occurence of the max value\n",
    "    \n",
    "    return(columnA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_agent_minimax_lookahead2_pruned(gameboard):\n",
    "\n",
    "    thresholdA = MAX\n",
    "    Qstate_valueA = [0.0 for i in range(Xdim)]\n",
    "    for columnA in range(Xdim):\n",
    "        gameboardA = gameboard.copy()\n",
    "        if gameboardA[columnA,-1]:\n",
    "            Qstate_valueA[columnA]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboardA[columnA]).index(0) # Find the lowest open slot in the chosen column\n",
    "            gameboardA[columnA,row] = 1 \n",
    "            if (check_for_win(gameboardA,columnA,row)) or (0 not in gameboardA):\n",
    "                Qstate_valueA[columnA] = get_Qvalue(gameboardA)\n",
    "                #if (check_for_win(gameboardA,columnA,row)):\n",
    "                    #Qstate_valueA[columnA] = MAX/2\n",
    "                #break # No need to evaluate deeper if this terminates the game\n",
    "            else:\n",
    "                 \n",
    "                thresholdB = MAX\n",
    "                Qstate_valueB = [0.0 for i in range(Xdim)]\n",
    "                for columnB in range(Xdim):\n",
    "                    gameboardB = -1*gameboardA.copy()\n",
    "                    if gameboardB[columnB,-1]:\n",
    "                        Qstate_valueB[columnB] = -MAX # don't allow an illegal move into a full column\n",
    "                    else:\n",
    "                        row = list(gameboardB[columnB]).index(0) # Find the lowest open slot in the chosen column\n",
    "                        gameboardB[columnB,row] = 1                        \n",
    "                        if (check_for_win(gameboardB,columnB,row)) or (0 not in gameboardB):\n",
    "                            Qstate_valueB[columnB] = get_Qvalue(gameboardB)\n",
    "                            #if (check_for_win(gameboardB,columnB,row)):\n",
    "                                #Qstate_valueB[columnB] = MAX/3\n",
    "                            #break # No need to evaluate deeper if this terminates the game\n",
    "                        else:\n",
    "                        \n",
    "                            Qstate_valueC = [0.0 for i in range(Xdim)]\n",
    "                            for columnC in range(Xdim):\n",
    "                                gameboardC = -1*gameboardB.copy()\n",
    "                                if gameboardC[columnC,-1]:\n",
    "                                    Qstate_valueC[columnC] = -MAX # don't allow an illegal move into a full column\n",
    "                                else:\n",
    "                                    row = list(gameboardC[columnC]).index(0) # Find the lowest open slot in the chosen column\n",
    "                                    gameboardC[columnC,row] = 1\n",
    "                                    if (check_for_win(gameboardC,columnC,row)):\n",
    "                                        Qstate_valueC[columnC] = get_Qvalue(gameboardC)\n",
    "                                        #Qstate_valueC[columnC] = MAX/4\n",
    "                                    else:\n",
    "                                        Qstate_valueC[columnC]= get_Qvalue(gameboardC)\n",
    "                                # End of loop of each value of columnC \n",
    "                                if (Qstate_valueC[columnC] > thresholdB): # If even one of B's options is higher, we don't need to evaluate further\n",
    "                                    break #PRUNING\n",
    "                            # This runs at the end of the columnC loop        \n",
    "                            #columnCmax = Qstate_valueB.index(max(Qstate_valueC))\n",
    "                            Qstate_valueC_max = max(Qstate_valueC)\n",
    "                            if (Qstate_valueC_max<thresholdB):\n",
    "                                thresholdB = Qstate_valueC_max\n",
    "                            Qstate_valueB[columnB] = -1*Qstate_valueC_max # Invert the opponent's gain to be our loss\n",
    "                    \n",
    "                    # End of loop of each value of columnB\n",
    "                    if (Qstate_valueB[columnB] > thresholdA): # If even one of B's options is higher, we don't need to evaluate further\n",
    "                        break #PRUNING       \n",
    "                \n",
    "                # This runs at the end of the columnB loop of all values of columnB\n",
    "                #columnBmax = Qstate_valueB.index(max(Qstate_valueB)) #opponent maximizes their Qvalue for each option\n",
    "                Qstate_valueB_max = max(Qstate_valueB)\n",
    "                if (Qstate_valueB_max<thresholdA):\n",
    "                    thresholdA = Qstate_valueB_max\n",
    "                Qstate_valueA[columnA] = -1*Qstate_valueB_max # Invert the opponent's gain to be our loss\n",
    "\n",
    "    count = Qstate_valueA.count(max(Qstate_valueA))\n",
    "    if count>1:\n",
    "        columnA = random.randint(0,Xdim-1)\n",
    "        while Qstate_valueA[columnA] != max(Qstate_valueA):\n",
    "            columnA=random.randint(0,Xdim-1)\n",
    "    else:\n",
    "        columnA = Qstate_valueA.index(max(Qstate_valueA)) # Find the first/only occurence of the max value\n",
    "    \n",
    "    return(columnA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_agent_minimax_lookahead3_pruned(gameboard):\n",
    "\n",
    "    thresholdA = MAX\n",
    "    Qstate_valueA = [0.0 for i in range(Xdim)]\n",
    "    for columnA in range(Xdim):\n",
    "        gameboardA = gameboard.copy()\n",
    "        if gameboardA[columnA,-1]:\n",
    "            Qstate_valueA[columnA]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboardA[columnA]).index(0) # Find the lowest open slot in the chosen column\n",
    "            gameboardA[columnA,row] = 1 \n",
    "            if (check_for_win(gameboardA,columnA,row)) or (0 not in gameboardA):\n",
    "                Qstate_valueA[columnA] = get_Qvalue(gameboardA)\n",
    "                #if (check_for_win(gameboardA,columnA,row)):\n",
    "                    #Qstate_valueA[columnA] = MAX/2\n",
    "                ## No need to evaluate deeper if this terminates the game\n",
    "            else:\n",
    "                 \n",
    "                thresholdB = MAX\n",
    "                Qstate_valueB = [0.0 for i in range(Xdim)]\n",
    "                for columnB in range(Xdim):\n",
    "                    gameboardB = -1*gameboardA.copy()\n",
    "                    if gameboardB[columnB,-1]:\n",
    "                        Qstate_valueB[columnB] = -MAX # don't allow an illegal move into a full column\n",
    "                    else:\n",
    "                        row = list(gameboardB[columnB]).index(0) # Find the lowest open slot in the chosen column\n",
    "                        gameboardB[columnB,row] = 1                        \n",
    "                        if (check_for_win(gameboardB,columnB,row)) or (0 not in gameboardB):\n",
    "                            Qstate_valueB[columnB] = get_Qvalue(gameboardB)\n",
    "                            #if (check_for_win(gameboardB,columnB,row)):\n",
    "                                #Qstate_valueB[columnB] = MAX/3\n",
    "                            ## No need to evaluate deeper if this terminates the game\n",
    "                        else:\n",
    "                        \n",
    "                            thresholdC = MAX\n",
    "                            Qstate_valueC = [0.0 for i in range(Xdim)]\n",
    "                            for columnC in range(Xdim):\n",
    "                                gameboardC = -1*gameboardB.copy()\n",
    "                                if gameboardC[columnC,-1]:\n",
    "                                    Qstate_valueC[columnC] = -MAX # don't allow an illegal move into a full column\n",
    "                                else:\n",
    "                                    row = list(gameboardC[columnC]).index(0) # Find the lowest open slot in the chosen column\n",
    "                                    gameboardC[columnC,row] = 1\n",
    "                                    if (check_for_win(gameboardC,columnC,row) or (0 not in gameboard)):\n",
    "                                        Qstate_valueC[columnC] = get_Qvalue(gameboardC)\n",
    "                                        #if (check_for_win(gameboardB,columnB,row)):\n",
    "                                            #Qstate_valueB[columnB] = MAX/4\n",
    "                                        ## No need to evaluate deeper if this terminates the game\n",
    "                                    else:   \n",
    "                                        \n",
    "                                        thresholdD = MAX\n",
    "                                        Qstate_valueD = [0.0 for i in range(Xdim)]\n",
    "                                        for columnD in range(Xdim):\n",
    "                                            gameboardD = -1*gameboardC.copy()\n",
    "                                            if gameboardD[columnD,-1]:\n",
    "                                                Qstate_valueD[columnD] = -MAX # don't allow an illegal move into a full column\n",
    "                                            else:\n",
    "                                                row = list(gameboardD[columnD]).index(0) # Find the lowest open slot in the chosen column\n",
    "                                                gameboardC[columnD,row] = 1\n",
    "                                                if (check_for_win(gameboardD,columnD,row) or 0 not in gameboard):\n",
    "                                                    Qstate_valueD[columnD] = get_Qvalue(gameboardD)\n",
    "                                                    #Qstate_valueC[columnD] = MAX/5\n",
    "                                                else:\n",
    "                                                    Qstate_valueD[columnD]= get_Qvalue(gameboardD)\n",
    "\n",
    "                                           # End of loop of each value of columnD \n",
    "                                            if (Qstate_valueD[columnD] > thresholdC): # If even one of B's options is higher, we don't need to evaluate further\n",
    "                                                break #PRUNING\n",
    "                                        # This runs at the end of the columnC loop        \n",
    "                                        #columnCmax = Qstate_valueB.index(max(Qstate_valueC))\n",
    "                                        Qstate_valueD_max = max(Qstate_valueD)\n",
    "                                        if (Qstate_valueD_max<thresholdC):\n",
    "                                            thresholdC = Qstate_valueD_max\n",
    "                                        Qstate_valueC[columnC] = -1*Qstate_valueD_max # Invert the opponent's gain to be our loss                                        \n",
    "                                        \n",
    "                                # End of loop of each value of columnC \n",
    "                                if (Qstate_valueC[columnC] > thresholdB): # If even one of B's options is higher, we don't need to evaluate further\n",
    "                                    break #PRUNING\n",
    "                            # This runs at the end of the columnC loop        \n",
    "                            #columnCmax = Qstate_valueB.index(max(Qstate_valueC))\n",
    "                            Qstate_valueC_max = max(Qstate_valueC)\n",
    "                            if (Qstate_valueC_max<thresholdB):\n",
    "                                thresholdB = Qstate_valueC_max\n",
    "                            Qstate_valueB[columnB] = -1*Qstate_valueC_max # Invert the opponent's gain to be our loss\n",
    "                    \n",
    "                    # End of loop of each value of columnB\n",
    "                    if (Qstate_valueB[columnB] > thresholdA): # If even one of B's options is higher, we don't need to evaluate further\n",
    "                        break #PRUNING                     \n",
    "                # This runs at the end of the columnB loop of all values of columnB\n",
    "                #columnBmax = Qstate_valueB.index(max(Qstate_valueB)) #opponent maximizes their Qvalue for each option\n",
    "                Qstate_valueB_max = max(Qstate_valueB)\n",
    "                if (Qstate_valueB_max<thresholdA):\n",
    "                    thresholdA = Qstate_valueB_max\n",
    "                Qstate_valueA[columnA] = -1*Qstate_valueB_max # Invert the opponent's gain to be our loss\n",
    "\n",
    "    count = Qstate_valueA.count(max(Qstate_valueA))\n",
    "    if count>1:\n",
    "        columnA = random.randint(0,Xdim-1)\n",
    "        while Qstate_valueA[columnA] != max(Qstate_valueA):\n",
    "            columnA=random.randint(0,Xdim-1)\n",
    "    else:\n",
    "        columnA = Qstate_valueA.index(max(Qstate_valueA)) # Find the first/only occurence of the max value\n",
    "    \n",
    "    return(columnA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAY THE MINIMAX AGENT AGAINST THE RANDOM AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerA = trained_agent_minimax_lookahead3_pruned\n",
    "playerB = random_agent_with_winblocklosssearch_middlestart\n",
    "test_games=10000\n",
    "test_games= 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-19 15:39:11.563769\n"
     ]
    }
   ],
   "source": [
    " print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Trainings_500Ktraining/models4/dqn_training_rewardEveryMove-Copy\\model-499999.cptk\n",
      "Model loaded\n",
      "\n",
      "PlayerA is a <function trained_agent_minimax_lookahead3_pruned at 0x000001F5B214AEA0>\n",
      "PlayerB is a <function random_agent_with_winblocklosssearch_middlestart at 0x000001F5A3A8FBF8>\n",
      "\n",
      "PlayerA won 2926 games for 58.52%\n",
      "PlayerA went first 51.52% of the time and won 1708 of those games for 66.3%\n",
      "PlayerA started in the middle 2576 times out of 2576 (100.0%)\n",
      "PlayerA won 2926 times out of 8187 chances (35.7%)\n",
      "PlayerA blocked 7610 times out of 8257 chances (92.2%)\n",
      "PlayerA created 51 opportunities for PlayerB\n",
      "PlayerA distribution = [6624, 8447, 13886, 12691, 10623, 7149, 6107]\n",
      "\n",
      "PlayerB won 1797 games for 35.94%\n",
      "PlayerB went first 48.48% of the time and won 1024 of those games for 42.2%\n",
      "PlayerB started in the middle 2424 times out of 2424 (100.0%)\n",
      "PlayerB won 1797 times out of 1797 chances (100.0%)\n",
      "PlayerB blocked 19974 times out of 19974 chances (100.0%)\n",
      "PlayerB created 0 opportunities for PlayerA\n",
      "PlayerB distribution = [8391, 8461, 8256, 10153, 9588, 11006, 8988]\n",
      "\n",
      "Player1 won 2732 games for 54.64%\n",
      "Player2 won 1991 games for 39.82%\n",
      "277 tied games for 5.54%\n"
     ]
    }
   ],
   "source": [
    "#This is up-to-date altered\n",
    "#\n",
    "# Play a set of matches\n",
    "#\n",
    "global player1_victories\n",
    "global player2_victories\n",
    "player1_victories = 0\n",
    "player2_victories = 0\n",
    "playerA_victories = 0\n",
    "playerB_victories = 0\n",
    "playerA_possible_wins = 0\n",
    "playerB_possible_wins = 0\n",
    "playerA_missed_wins = 0\n",
    "playerB_missed_wins = 0\n",
    "playerA_possible_blocks = 0\n",
    "playerB_possible_blocks = 0\n",
    "playerA_missed_blocks = 0\n",
    "playerB_missed_blocks = 0\n",
    "playerA_created_possible_losses = 0\n",
    "playerB_created_possible_losses = 0\n",
    "playerA_starts = 0\n",
    "playerB_starts = 0\n",
    "playerA_middle_starts = 0\n",
    "playerB_middle_starts = 0\n",
    "playerA_firstplayerwins = 0\n",
    "playerB_firstplayerwins = 0\n",
    "tied_games=0\n",
    "gameboard_history = np.zeros((test_games,Xdim,Ydim),dtype=int) # use this to use gameboard_history to record all final gameboards\n",
    "playerA_distribution = [0]*Xdim\n",
    "playerB_distribution = [0]*Xdim\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "  saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "  print('Model loaded')\n",
    "    \n",
    "  for game in range(test_games):   \n",
    "    # reset gameboard\n",
    "    gameboard = np.zeros((Xdim,Ydim),dtype=int) \n",
    "    #gameboard_history = np.zeros((Xdim*Ydim,Xdim,Ydim),dtype=int) # Use this to use gameboard_history to record steps in a game\n",
    "    observations = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) #new\n",
    "    observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "    done = False\n",
    "       \n",
    "    playerA_first = (random.random()<0.5)\n",
    "    if playerA_first:\n",
    "        playerA_starts+=1\n",
    "    else:\n",
    "        playerB_starts+=1\n",
    "    episode_steps = 0    \n",
    "    while not done and 0 in gameboard:\n",
    "        \n",
    "        #Player A\n",
    "        if (episode_steps>0) or playerA_first:\n",
    "            gameboard *=-1 # switch sides every turn \n",
    "            episode_steps+=1\n",
    "            # Determine our move\n",
    "            column=playerA(gameboard)\n",
    "            playerA_distribution[column]+=1 # deleteme\n",
    "            #display_gameboard(gameboard) # deleteme\n",
    "            #print('trained', column) # deleteme\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column \n",
    "            gameboard[column,row] = 1 \n",
    "            # Update our stats\n",
    "            done, playerA_victories, playerA_possible_wins, playerA_missed_wins, \\\n",
    "                playerA_possible_blocks, playerA_missed_blocks, playerA_created_possible_losses, playerA_middle_starts\\\n",
    "                = update_stats(gameboard,column, row,episode_steps, playerA_victories, playerA_possible_wins, playerA_missed_wins, \\\n",
    "                playerA_possible_blocks, playerA_missed_blocks, playerA_created_possible_losses, playerA_middle_starts)\n",
    "            if done:\n",
    "                gameboard_history[game] = gameboard\n",
    "                if playerA_first:\n",
    "                    playerA_firstplayerwins +=1\n",
    "                break # PlayerA won!\n",
    "        \n",
    "        # Check for a tie\n",
    "        if (0 not in gameboard):\n",
    "            tied_games+=1\n",
    "            gameboard_history[game] = gameboard\n",
    "            break\n",
    "            \n",
    "        #PlayerB        \n",
    "        gameboard *=-1 # switch sides every turn \n",
    "        episode_steps+=1\n",
    "        # Determine our move\n",
    "        #display_gameboard(gameboard) # deleteme\n",
    "        column=playerB(gameboard)\n",
    "        playerB_distribution[column]+=1 # deleteme\n",
    "        row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column \n",
    "        gameboard[column,row] = 1 \n",
    "        # Update our stats\n",
    "        done, playerB_victories, playerB_possible_wins, playerB_missed_wins, \\\n",
    "            playerB_possible_blocks, playerB_missed_blocks, playerB_created_possible_losses, playerB_middle_starts\\\n",
    "            = update_stats(gameboard,column, row,episode_steps, playerB_victories, playerB_possible_wins, playerB_missed_wins, \\\n",
    "            playerB_possible_blocks, playerB_missed_blocks, playerB_created_possible_losses, playerB_middle_starts)\n",
    "        if done:\n",
    "            gameboard_history[game] = gameboard\n",
    "            if not playerA_first:\n",
    "                playerB_firstplayerwins +=1\n",
    "            #print('playerB wins!')  # deleteme\n",
    "            break # PlayerB won!\n",
    "            \n",
    "        # Check for a tie\n",
    "        if (0 not in gameboard):\n",
    "            tied_games+=1\n",
    "            gameboard_history[game] = gameboard\n",
    "            break\n",
    "     \n",
    "    if (TRAIN_AFTER_TESTGAMES):\n",
    "        # At the end of each game, Get a random batch of experiences.\n",
    "        trainBatch = myBuffer.sample(batch_size) \n",
    "        # Below we perform the Double-DQN update to the target Q-values\n",
    "        Q1 = sess.run(mainQN.predict, feed_dict={mainQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "        Q2 = sess.run(targetQN.q_out, feed_dict={targetQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "        end_multiplier = -(trainBatch[:,4] - 1)\n",
    "        doubleQ = Q2[range(batch_size),Q1]\n",
    "        targetQ = trainBatch[:,2] + (targetQ_y*doubleQ * end_multiplier)\n",
    "        # Update the network with our target values.\n",
    "        _, q_loss = sess.run([mainQN.update, mainQN.loss],\n",
    "            feed_dict={mainQN.observation_input:np.stack(trainBatch[:,0], axis=0),\n",
    "                    mainQN.targetQ:targetQ, \n",
    "                    mainQN.actions:trainBatch[:,1]})\n",
    "        losses.append(q_loss)\n",
    "\n",
    "print('\\nPlayerA is a',playerA)\n",
    "print('PlayerB is a',playerB)\n",
    "print(f'\\nPlayerA won {playerA_victories} games for {100*playerA_victories/test_games}%')\n",
    "print(f'PlayerA went first {100*playerA_starts/test_games}% of the time and won {playerA_firstplayerwins} of those games for {round(10*100*playerA_firstplayerwins/max(1,playerA_starts))/10}%')\n",
    "print(f'PlayerA started in the middle {playerA_middle_starts} times out of {playerA_starts} ({round(10*100*playerA_middle_starts/max(1,playerA_starts))/10}%)')\n",
    "print(f'PlayerA won {playerA_victories} times out of {playerA_possible_wins} chances ({round(10*100*playerA_victories/max(1,playerA_possible_wins))/10}%)')\n",
    "print(f'PlayerA blocked {playerA_possible_blocks-playerA_missed_blocks} times out of {playerA_possible_blocks} chances ({round(10*100*(1-playerA_missed_blocks/max(1,playerA_possible_blocks)))/10}%)')\n",
    "print(f'PlayerA created {playerA_created_possible_losses} opportunities for PlayerB')\n",
    "print(f'PlayerA distribution = {playerA_distribution}')\n",
    "print(f'\\nPlayerB won {playerB_victories} games for {100*playerB_victories/test_games}%')\n",
    "print(f'PlayerB went first {100*playerB_starts/test_games}% of the time and won {playerB_firstplayerwins} of those games for {round(10*100*playerB_firstplayerwins/max(1,playerB_starts))/10}%')\n",
    "print(f'PlayerB started in the middle {playerB_middle_starts} times out of {playerB_starts} ({round(10*100*playerB_middle_starts/max(1,playerB_starts))/10}%)')\n",
    "print(f'PlayerB won {playerB_victories} times out of {playerB_possible_wins} chances ({round(10*100*playerB_victories/max(1,playerB_possible_wins))/10}%)')\n",
    "print(f'PlayerB blocked {playerB_possible_blocks-playerB_missed_blocks} times out of {playerB_possible_blocks} chances ({round(10*100*(1-playerB_missed_blocks/max(1,playerB_possible_blocks)))/10}%)')\n",
    "print(f'PlayerB created {playerB_created_possible_losses} opportunities for PlayerA')\n",
    "print(f'PlayerB distribution = {playerB_distribution}')\n",
    "print(f'\\nPlayer1 won {player1_victories} games for {100*player1_victories/test_games}%')\n",
    "print(f'Player2 won {player2_victories} games for {100*player2_victories/test_games}%')\n",
    "print(f'{tied_games} tied games for {100*tied_games/test_games}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size= 64\n",
      "update_freq =  8\n",
      "targetQ_y= 0.4\n",
      "training_games = 1\n",
      "annealing_time = 0.6\n",
      "number of units in hidden layer =  512\n",
      "learning_rate= 0.0003\n",
      "Mean Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('batch_size=', batch_size)\n",
    "print('update_freq = ',update_freq)\n",
    "print('targetQ_y=',targetQ_y)\n",
    "print('training_games =',training_games)\n",
    "print('annealing_time =', annealing_time)\n",
    "print('number of units in hidden layer = ',h_size)\n",
    "print('learning_rate=' ,learning_rate)\n",
    "print (\"Mean Reward: {}\".format(np.mean(episode_rewards[-50:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-19 19:48:45.864446\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerA = trained_agent_minimax_lookahead2_pruned\n",
    "playerB = random_agent_with_winblocklosssearch_middlestart\n",
    "test_games=10000\n",
    "test_games= 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-19 19:48:45.897520\n"
     ]
    }
   ],
   "source": [
    " print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Trainings_500Ktraining/models4/dqn_training_rewardEveryMove-Copy\\model-499999.cptk\n",
      "Model loaded\n",
      "\n",
      "PlayerA is a <function trained_agent_minimax_lookahead2_pruned at 0x000001F5B0A8ABF8>\n",
      "PlayerB is a <function random_agent_with_winblocklosssearch_middlestart at 0x000001F5A3A8FBF8>\n",
      "\n",
      "PlayerA won 4505 games for 90.1%\n",
      "PlayerA went first 49.8% of the time and won 2376 of those games for 95.4%\n",
      "PlayerA started in the middle 2490 times out of 2490 (100.0%)\n",
      "PlayerA won 4505 times out of 4978 chances (90.5%)\n",
      "PlayerA blocked 2587 times out of 2795 chances (92.6%)\n",
      "PlayerA created 95 opportunities for PlayerB\n",
      "PlayerA distribution = [4102, 8486, 10278, 12027, 10237, 4304, 2578]\n",
      "\n",
      "PlayerB won 374 games for 7.48%\n",
      "PlayerB went first 50.2% of the time and won 281 of those games for 11.2%\n",
      "PlayerB started in the middle 2510 times out of 2510 (100.0%)\n",
      "PlayerB won 374 times out of 374 chances (100.0%)\n",
      "PlayerB blocked 13388 times out of 13388 chances (100.0%)\n",
      "PlayerB created 0 opportunities for PlayerA\n",
      "PlayerB distribution = [6283, 6641, 6712, 8955, 7350, 7131, 6845]\n",
      "\n",
      "Player1 won 2657 games for 53.14%\n",
      "Player2 won 2222 games for 44.44%\n",
      "121 tied games for 2.42%\n"
     ]
    }
   ],
   "source": [
    "#This is up-to-date altered\n",
    "#\n",
    "# Play a set of matches\n",
    "#\n",
    "global player1_victories\n",
    "global player2_victories\n",
    "player1_victories = 0\n",
    "player2_victories = 0\n",
    "playerA_victories = 0\n",
    "playerB_victories = 0\n",
    "playerA_possible_wins = 0\n",
    "playerB_possible_wins = 0\n",
    "playerA_missed_wins = 0\n",
    "playerB_missed_wins = 0\n",
    "playerA_possible_blocks = 0\n",
    "playerB_possible_blocks = 0\n",
    "playerA_missed_blocks = 0\n",
    "playerB_missed_blocks = 0\n",
    "playerA_created_possible_losses = 0\n",
    "playerB_created_possible_losses = 0\n",
    "playerA_starts = 0\n",
    "playerB_starts = 0\n",
    "playerA_middle_starts = 0\n",
    "playerB_middle_starts = 0\n",
    "playerA_firstplayerwins = 0\n",
    "playerB_firstplayerwins = 0\n",
    "tied_games=0\n",
    "gameboard_history = np.zeros((test_games,Xdim,Ydim),dtype=int) # use this to use gameboard_history to record all final gameboards\n",
    "playerA_distribution = [0]*Xdim\n",
    "playerB_distribution = [0]*Xdim\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "  saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "  print('Model loaded')\n",
    "    \n",
    "  for game in range(test_games):   \n",
    "    # reset gameboard\n",
    "    gameboard = np.zeros((Xdim,Ydim),dtype=int) \n",
    "    #gameboard_history = np.zeros((Xdim*Ydim,Xdim,Ydim),dtype=int) # Use this to use gameboard_history to record steps in a game\n",
    "    observations = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) #new\n",
    "    observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "    done = False\n",
    "       \n",
    "    playerA_first = (random.random()<0.5)\n",
    "    if playerA_first:\n",
    "        playerA_starts+=1\n",
    "    else:\n",
    "        playerB_starts+=1\n",
    "    episode_steps = 0    \n",
    "    while not done and 0 in gameboard:\n",
    "        \n",
    "        #Player A\n",
    "        if (episode_steps>0) or playerA_first:\n",
    "            gameboard *=-1 # switch sides every turn \n",
    "            episode_steps+=1\n",
    "            # Determine our move\n",
    "            column=playerA(gameboard)\n",
    "            playerA_distribution[column]+=1 # deleteme\n",
    "            #display_gameboard(gameboard) # deleteme\n",
    "            #print('trained', column) # deleteme\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column \n",
    "            gameboard[column,row] = 1 \n",
    "            # Update our stats\n",
    "            done, playerA_victories, playerA_possible_wins, playerA_missed_wins, \\\n",
    "                playerA_possible_blocks, playerA_missed_blocks, playerA_created_possible_losses, playerA_middle_starts\\\n",
    "                = update_stats(gameboard,column, row,episode_steps, playerA_victories, playerA_possible_wins, playerA_missed_wins, \\\n",
    "                playerA_possible_blocks, playerA_missed_blocks, playerA_created_possible_losses, playerA_middle_starts)\n",
    "            if done:\n",
    "                gameboard_history[game] = gameboard\n",
    "                if playerA_first:\n",
    "                    playerA_firstplayerwins +=1\n",
    "                break # PlayerA won!\n",
    "        \n",
    "        # Check for a tie\n",
    "        if (0 not in gameboard):\n",
    "            tied_games+=1\n",
    "            gameboard_history[game] = gameboard\n",
    "            break\n",
    "            \n",
    "        #PlayerB        \n",
    "        gameboard *=-1 # switch sides every turn \n",
    "        episode_steps+=1\n",
    "        # Determine our move\n",
    "        #display_gameboard(gameboard) # deleteme\n",
    "        column=playerB(gameboard)\n",
    "        playerB_distribution[column]+=1 # deleteme\n",
    "        row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column \n",
    "        gameboard[column,row] = 1 \n",
    "        # Update our stats\n",
    "        done, playerB_victories, playerB_possible_wins, playerB_missed_wins, \\\n",
    "            playerB_possible_blocks, playerB_missed_blocks, playerB_created_possible_losses, playerB_middle_starts\\\n",
    "            = update_stats(gameboard,column, row,episode_steps, playerB_victories, playerB_possible_wins, playerB_missed_wins, \\\n",
    "            playerB_possible_blocks, playerB_missed_blocks, playerB_created_possible_losses, playerB_middle_starts)\n",
    "        if done:\n",
    "            gameboard_history[game] = gameboard\n",
    "            if not playerA_first:\n",
    "                playerB_firstplayerwins +=1\n",
    "            #print('playerB wins!')  # deleteme\n",
    "            break # PlayerB won!\n",
    "            \n",
    "        # Check for a tie\n",
    "        if (0 not in gameboard):\n",
    "            tied_games+=1\n",
    "            gameboard_history[game] = gameboard\n",
    "            break\n",
    "     \n",
    "    if (TRAIN_AFTER_TESTGAMES):\n",
    "        # At the end of each game, Get a random batch of experiences.\n",
    "        trainBatch = myBuffer.sample(batch_size) \n",
    "        # Below we perform the Double-DQN update to the target Q-values\n",
    "        Q1 = sess.run(mainQN.predict, feed_dict={mainQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "        Q2 = sess.run(targetQN.q_out, feed_dict={targetQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "        end_multiplier = -(trainBatch[:,4] - 1)\n",
    "        doubleQ = Q2[range(batch_size),Q1]\n",
    "        targetQ = trainBatch[:,2] + (targetQ_y*doubleQ * end_multiplier)\n",
    "        # Update the network with our target values.\n",
    "        _, q_loss = sess.run([mainQN.update, mainQN.loss],\n",
    "            feed_dict={mainQN.observation_input:np.stack(trainBatch[:,0], axis=0),\n",
    "                    mainQN.targetQ:targetQ, \n",
    "                    mainQN.actions:trainBatch[:,1]})\n",
    "        losses.append(q_loss)\n",
    "\n",
    "print('\\nPlayerA is a',playerA)\n",
    "print('PlayerB is a',playerB)\n",
    "print(f'\\nPlayerA won {playerA_victories} games for {100*playerA_victories/test_games}%')\n",
    "print(f'PlayerA went first {100*playerA_starts/test_games}% of the time and won {playerA_firstplayerwins} of those games for {round(10*100*playerA_firstplayerwins/max(1,playerA_starts))/10}%')\n",
    "print(f'PlayerA started in the middle {playerA_middle_starts} times out of {playerA_starts} ({round(10*100*playerA_middle_starts/max(1,playerA_starts))/10}%)')\n",
    "print(f'PlayerA won {playerA_victories} times out of {playerA_possible_wins} chances ({round(10*100*playerA_victories/max(1,playerA_possible_wins))/10}%)')\n",
    "print(f'PlayerA blocked {playerA_possible_blocks-playerA_missed_blocks} times out of {playerA_possible_blocks} chances ({round(10*100*(1-playerA_missed_blocks/max(1,playerA_possible_blocks)))/10}%)')\n",
    "print(f'PlayerA created {playerA_created_possible_losses} opportunities for PlayerB')\n",
    "print(f'PlayerA distribution = {playerA_distribution}')\n",
    "print(f'\\nPlayerB won {playerB_victories} games for {100*playerB_victories/test_games}%')\n",
    "print(f'PlayerB went first {100*playerB_starts/test_games}% of the time and won {playerB_firstplayerwins} of those games for {round(10*100*playerB_firstplayerwins/max(1,playerB_starts))/10}%')\n",
    "print(f'PlayerB started in the middle {playerB_middle_starts} times out of {playerB_starts} ({round(10*100*playerB_middle_starts/max(1,playerB_starts))/10}%)')\n",
    "print(f'PlayerB won {playerB_victories} times out of {playerB_possible_wins} chances ({round(10*100*playerB_victories/max(1,playerB_possible_wins))/10}%)')\n",
    "print(f'PlayerB blocked {playerB_possible_blocks-playerB_missed_blocks} times out of {playerB_possible_blocks} chances ({round(10*100*(1-playerB_missed_blocks/max(1,playerB_possible_blocks)))/10}%)')\n",
    "print(f'PlayerB created {playerB_created_possible_losses} opportunities for PlayerA')\n",
    "print(f'PlayerB distribution = {playerB_distribution}')\n",
    "print(f'\\nPlayer1 won {player1_victories} games for {100*player1_victories/test_games}%')\n",
    "print(f'Player2 won {player2_victories} games for {100*player2_victories/test_games}%')\n",
    "print(f'{tied_games} tied games for {100*tied_games/test_games}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size= 64\n",
      "update_freq =  8\n",
      "targetQ_y= 0.4\n",
      "training_games = 1\n",
      "annealing_time = 0.6\n",
      "number of units in hidden layer =  512\n",
      "learning_rate= 0.0003\n",
      "Mean Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('batch_size=', batch_size)\n",
    "print('update_freq = ',update_freq)\n",
    "print('targetQ_y=',targetQ_y)\n",
    "print('training_games =',training_games)\n",
    "print('annealing_time =', annealing_time)\n",
    "print('number of units in hidden layer = ',h_size)\n",
    "print('learning_rate=' ,learning_rate)\n",
    "print (\"Mean Reward: {}\".format(np.mean(episode_rewards[-50:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-19 20:39:37.966383\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT: ADDING WINLOSS SEARCH AND MIDDLESTART INCREASED THE FIRST-WIN PERCENTAGE TO OVER 92% (A 3-4% INCREASE, JUST OUTSIDE THE TYPICAL RANDOM VARIANCE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RE-RAN THE FULL-TRAINING (EXTRA BLOCKING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT: ADDING WINLOSS SEARCH TO THE FULLY TRAINED MINIMAX AGENT ONLY ADDED +5% TO FIRST-MOVE-WINS, STILL UNDER 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT:  THE ADDITIONAL BLOCK TRAINING HELPED THE MINIMAX PLAYER A LOT. NOTICE THAT THE MIDDLESTARTS ARE UP 100% COMPARED TO THE NON-MINIMAX PLAYER.  WIN-CONVERSIONS ARE DOWN A LITTLE, BUT BLOCK-CONVERSIONS ARE UP OVER 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT: ADDING MIDDLESTART TO THE MININMAX PLAYER HELPS BUT NOT AS MUCH AS IT HELPED THE PLAYER W/O MINIMAX\n",
    "# (ABOUT +4% IN FIRST-MOVE-WINS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDING MINIMAX TO PRETRAINING HAS ONLY MARGINAL EFFECT ON THE STATS, A SLIGHTLY BETTER WIN PERCENTAGE, BUT MAYBE IN THE MARGIN OF STATISTICAL VARIANCE\n",
    "# EVEN IF WE INCLUDE WINSEARCH, IT DOESN'T HELP MUCH, AND THE BLOCK PERCENTAGE ACTUALLY GOES DOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOING FIRST IS AN ADVANTAGE (+10% WINS) FOR A RANDOM AGENT WITH WINLOSSSEARCH AND MIDDLESTART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOING FIRST IS A SMALLER ADVANTAGE (+5.7% WINS) FOR A RANDOM AGENT WITH WINLOSSSEARCH WITHOUT MIDDLESTART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STARTING IN THE MIDDLE IS AN ADVANTAGE (+8.8% WINS) FOR A RANDOM AGENT WITH WINLOSSSEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STARTING FIRST IS A HUGE ADVANTAGE (+64.4% WINS) FOR TRAINED AGENTS (TRAINED TO CHOOSE THE MIDDLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimax agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-19 20:39:37.981638\n"
     ]
    }
   ],
   "source": [
    " print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = 500  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result: A trained agent with minimax beats a trained agent 61% of the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result: On the other hand, a trained agent with minimax does LESS well against a random agent with winlosssearch than\n",
    "    a trained agent without minimax would do against the same opponent.  All stats worsen.( Is this still true?)\n",
    "    \n",
    "    This could be because the current training doesn't do a good job searching for wins, so it isn't a good estimate of what\n",
    "    the agent with winlosssearch will do.  Conversely, it is a good predictor of what the trained agent (without minimax)\n",
    "    will do, so the minimax works against only the trained agent who uses the same training.\n",
    "    \n",
    "    Note that the time for 1000 matches went up from 1:51 (for the trained agent) to 5:02 (for the trained agent with \n",
    "    minimax), so there is significant time increase with only one look-ahead step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-57-66e34c7808dc>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-57-66e34c7808dc>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break\n",
    "How do we avoid re-playing the same few games?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LET THE USER PLAY AGAINST THE COMPUTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIXED TRAINING + NO PRETRAINING ON WINS AND LOSSES \n",
    "# DIMINISHING LEARNING RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to play a game? (y/n)n\n",
      "OK, we'll play later.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let the user play a set of games against the computer\n",
    "#\n",
    "def get_user_input(gameboard):\n",
    "    display_gameboard(gameboard)\n",
    "    user_string= input('Pick a column to play (1-7)  ')\n",
    "    while user_string[0] not in ['1','2','3','4','5','6','7']:\n",
    "        display_game_board(gameboard)\n",
    "        print('Illegal entry.  Pick a column (1-7)  ')\n",
    "    column=int(user_string[0])-1\n",
    "    if gameboard[column,-1]:\n",
    "        print(\"That column is already full.  I'll pick for you this time.\")\n",
    "        while gameboard[column,-1]:\n",
    "            column = random.choice(range(Xdim))\n",
    "    return(column)\n",
    "\n",
    "\n",
    "computer_player = random_agent_with_winlosssearch_middlestart\n",
    "play_yes_no = 'invalid'\n",
    "done=0\n",
    "while (play_yes_no[0] != 'n'):\n",
    "    while (play_yes_no[0] != 'n') and (play_yes_no[0] != 'y'):\n",
    "        play_yes_no = input('Would you like to play a game? (y/n)')\n",
    "    if (play_yes_no[0]== 'y'):\n",
    "        play_yes_no = 'invalid' # Loop back after the game to see if we play another game\n",
    "        gameboard = np.zeros((Xdim,Ydim),dtype=int) \n",
    "        while (done==0 and 0 in gameboard):\n",
    "            # User turn\n",
    "            user_column = get_user_input(gameboard)\n",
    "            user_row = list(gameboard[user_column]).index(0)\n",
    "            gameboard[user_column,user_row]=1\n",
    "            if check_for_win(gameboard,user_column,user_row):\n",
    "                display_gameboard(gameboard)\n",
    "                print('You Win!')\n",
    "                done=1\n",
    "            gameboard = -1*gameboard\n",
    "            #\n",
    "            # Computer turn\n",
    "            if (done==0 and 0 in gameboard):\n",
    "                computer_column = computer_player(gameboard)\n",
    "                computer_row = list(gameboard[computer_column]).index(0)\n",
    "                gameboard[computer_column,computer_row]=1\n",
    "                if check_for_win(gameboard,computer_column,computer_row):\n",
    "                    display_gameboard(-1*gameboard)\n",
    "                    print('Too Bad!')\n",
    "                    done=1\n",
    "                gameboard = -1*gameboard\n",
    "        \n",
    "        if (done==0 and 0 not in gameboard): # Tie game\n",
    "            display_gameboard(gameboard)\n",
    "            print('Tie game')\n",
    "          \n",
    "print(\"OK, we'll play later.\") # No more games\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-59-664a8a4ad712>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-59-664a8a4ad712>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    Try training with\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "break\n",
    "Try training with \n",
    "All possible wins\n",
    "Start in middle column\n",
    "Always take a win if possible\n",
    "Always block a win if possible\n",
    "Look ahead 2,4,6,8 moves\n",
    "\n",
    "check to see if it learns:\n",
    "To start in the middle column\n",
    "To always take a win if possible\n",
    "To always block a win if possible#\n",
    "To avoid creating opportunities\n",
    "To always win as player1\n",
    "Games should get longer between perfect players\n",
    "\n",
    "Try varying\n",
    "number of nodes in the hidden layer h_size = 2K\n",
    "How quickly to shift from training to exploitation  annealing_steps\n",
    "Learning Rate\n",
    "batch_size of experiences to train=32\n",
    "targetQ_y\n",
    "update_freq  How often to train\n",
    "reward structure: rewards only at the last move, retroactive rewards throughout the game (vary the decay), mirrored plus/minus rewards\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "steps_per_update = [2,4,8,16,32,64,128]\n",
    "\n",
    "overall_wins = [57.9,62.0, 60.5, 57.8, 58.7, 54.8, 50.8]\n",
    "ties = [7.65, 10,6.2,4.1,5.3,3.7,3.5]\n",
    "wins_plus_half_ties = np.add(overall_wins,np.multiply(0.5,ties))\n",
    "first_player_wins = [64.6, 73.2,76.1,73.7,66.9,65.4,55.2]\n",
    "win_conversion = [82.7,87.4,91.2, 92.1, 81.4, 81.6, 73.8]\n",
    "block_conversion = [88.1, 92.8, 83.8,78.0,82.2,75.4,73.4]\n",
    "\n",
    "plt.plot(steps_per_update,overall_wins, color= \"red\", linestyle= \"solid\",label=\"Overall wins\")\n",
    "plt.plot(steps_per_update,wins_plus_half_ties, color= \"red\", linestyle=\"dashed\",label=\"Wins + Ties/2\")\n",
    "plt.plot(steps_per_update,first_player_wins, color= \"blue\", linestyle=\"solid\",label=\"First Player Wins\")\n",
    "plt.plot(steps_per_update,win_conversion, color= \"grey\", linestyle=\"dotted\",label=\"Win Conversion\")\n",
    "plt.plot(steps_per_update,block_conversion, color= \"grey\", linestyle=\"dashdot\",label=\"Block Conversion\")\n",
    "\n",
    "plt.title(\"PERFORMANCE VS. UPDATE FREQUENCY\")\n",
    "plt.xlabel(\"Moves Per Training Update\")\n",
    "plt.ylabel(\"Performance %\")\n",
    "#plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "#plt.savefig('PerformanceVsUpdateFrequency.png')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
